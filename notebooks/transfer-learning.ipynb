{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will guide you through the process of using _transfer learning_ to learn an accurate image classifier from a small number of training samples. Transfer learning refers to the process of taking an existing neural network which was previously trained to good performance on a larger dataset, and using it as the basis for a new model which leverages the previous model's accuracy for the new task. Another name for this procedure is called \"fine-tuning\" because we will take a previous neural network and fine-tune its weights to a new dataset.\n",
    "\n",
    "How it works: we will first load a previously-trained neural net, Keras's built-in VGG16 model which was trained to do ImageNet classification on 1000 classes, and remove its final layer, the 1000-neuron softmax classification layer, and replace it with a new classification layer for the number of classes we are training on. We then \"freeze\" all the weights in the network except the ones connecting to the new classification layer, and then re-train the model on our new dataset. By keeping the earlier weights fixed, we get to use the features that were discoverd in the previous model.\n",
    "\n",
    "We will compare using this method to training a small neural network from scratch on the new dataset, and as we shall see, it will dramatically improve our accuracy.\n",
    "\n",
    "#http://stackoverflow.com/questions/41378461/how-to-use-models-from-keras-applications-for-transfer-learnig/41386444#41386444"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "WARNING (theano.sandbox.cuda): The cuda backend is deprecated and will be removed in the next release (v0.10).  Please switch to the gpuarray backend. You can get more information about how to switch at this URL:\n",
      " https://github.com/Theano/Theano/wiki/Converting-to-the-new-gpu-back-end%28gpuarray%29\n",
      "\n",
      "Using gpu device 0: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5103)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=gpu,floatX=float32\"\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.imagenet_utils import preprocess_input\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    train, test, val split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished loading 6209 images from 97 categories\n",
      "train / validation / test split: 4346, 931, 932\n",
      "('training data shape: ', (4346, 224, 224, 3))\n",
      "('training labels shape: ', (4346, 97))\n"
     ]
    }
   ],
   "source": [
    "root = '../data/101_ObjectCategories'\n",
    "exclude = ['BACKGROUND_Google', 'Motorbikes', 'airplanes', 'Faces_easy', 'Faces']\n",
    "train_split, val_split = 0.7, 0.15\n",
    "\n",
    "categories = [x[0] for x in os.walk(root) if x[0]][1:]\n",
    "categories = [c for c in categories \n",
    "              if c not in [os.path.join(root, e) for e in exclude]]\n",
    "\n",
    "# helper function to load image and return it and input vector\n",
    "def get_image(path):\n",
    "    img = image.load_img(path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    return img, x\n",
    "\n",
    "# load all the images from root folder\n",
    "data = []\n",
    "for c, category in enumerate(categories):\n",
    "    images = [os.path.join(dp, f) for dp, dn, filenames \n",
    "              in os.walk(category) for f in filenames \n",
    "              if os.path.splitext(f)[1].lower() in ['.jpg','.png','.jpeg']]\n",
    "    for img_path in images:\n",
    "        img, x = get_image(img_path)\n",
    "        data.append({'x':np.array(x[0]), 'y':c})\n",
    "\n",
    "# count the number of classes\n",
    "num_classes = len(categories)\n",
    "\n",
    "# shuffle \n",
    "random.shuffle(data)\n",
    "\n",
    "# create training / validation / test split (70%, 15%, 15%)\n",
    "idx_val = int(train_split * len(data))\n",
    "idx_test = int((train_split + val_split) * len(data))\n",
    "train = data[:idx_val]\n",
    "val = data[idx_val:idx_test]\n",
    "test = data[idx_test:]\n",
    "\n",
    "# separate data for labels\n",
    "x_train, y_train = np.array([t[\"x\"] for t in train]), [t[\"y\"] for t in train]\n",
    "x_val, y_val = np.array([t[\"x\"] for t in val]), [t[\"y\"] for t in val]\n",
    "x_test, y_test = np.array([t[\"x\"] for t in test]), [t[\"y\"] for t in test]\n",
    "\n",
    "# normalize data\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_val = x_val.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "\n",
    "# convert labels to one-hot vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# summary\n",
    "print(\"finished loading %d images from %d categories\"%(len(data), num_classes))\n",
    "print(\"train / validation / test split: %d, %d, %d\"%(len(x_train), len(x_val), len(x_test)))\n",
    "print(\"training data shape: \", x_train.shape)\n",
    "print(\"training labels shape: \", y_train.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly look at a few sample images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to build a neural network from scratch for doing classification on our dataset. This will give us a baseline to compare our fine-tuned network later.\n",
    "\n",
    "This network will contain 4 convolutional and max-pooling layers, followed by a dropout after every other conv/pooling pair. After the last pooling layer, we will attach a fully-connected layer with 256 neurons, another dropout layer, then finally a softmax classification layer for our classes.\n",
    "\n",
    "Our loss function will be, as usual, `categorical_crossentropy` loss, and our learning algorithm will be adadelta. Various things about this network can be changed to get better performance, perhaps using a larger network or a different optimizer will help, but for the purposes of this notebook, the goal is to just get an understanding of an approximate baseline for comparison's sake.\n",
    "\n",
    "Upon compiling the network, let's run `model.summary()` to get a snapshot of its layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 222, 222, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 111, 111, 32)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 109, 109, 32)      9248      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 109, 109, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 54, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 54, 54, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 52, 52, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 52, 52, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               1179904   \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 97)                24929     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 97)                0         \n",
      "=================================================================\n",
      "Total params: 1,233,473.0\n",
      "Trainable params: 1,233,473.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've created a medium-sized network with >1.2 million weights and biases. Most of them are leading into the one pre-softmax fully-connected layer \"dense_5\". \n",
    "\n",
    "We can now go ahead and train our model. We'll also record its history so we can plot the loss over time later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4346 samples, validate on 931 samples\n",
      "Epoch 1/50\n",
      "4346/4346 [==============================] - 38s - loss: 4.4247 - acc: 0.0624 - val_loss: 4.2618 - val_acc: 0.1042\n",
      "Epoch 2/50\n",
      "4346/4346 [==============================] - 38s - loss: 3.9450 - acc: 0.1505 - val_loss: 3.7565 - val_acc: 0.2148\n",
      "Epoch 3/50\n",
      "4346/4346 [==============================] - 38s - loss: 3.4341 - acc: 0.2423 - val_loss: 3.4071 - val_acc: 0.2524\n",
      "Epoch 4/50\n",
      "4346/4346 [==============================] - 38s - loss: 3.0378 - acc: 0.3095 - val_loss: 3.0780 - val_acc: 0.3330\n",
      "Epoch 5/50\n",
      "4346/4346 [==============================] - 38s - loss: 2.6101 - acc: 0.3873 - val_loss: 2.8803 - val_acc: 0.3695\n",
      "Epoch 6/50\n",
      "4346/4346 [==============================] - 38s - loss: 2.2116 - acc: 0.4653 - val_loss: 2.6636 - val_acc: 0.3985\n",
      "Epoch 7/50\n",
      "4346/4346 [==============================] - 38s - loss: 1.8960 - acc: 0.5251 - val_loss: 2.5740 - val_acc: 0.4275\n",
      "Epoch 8/50\n",
      "4346/4346 [==============================] - 38s - loss: 1.6027 - acc: 0.5835 - val_loss: 2.6306 - val_acc: 0.4168\n",
      "Epoch 9/50\n",
      "4346/4346 [==============================] - 38s - loss: 1.3423 - acc: 0.6387 - val_loss: 2.6260 - val_acc: 0.4329\n",
      "Epoch 10/50\n",
      "4346/4346 [==============================] - 38s - loss: 1.1145 - acc: 0.6933 - val_loss: 2.5236 - val_acc: 0.4565\n",
      "Epoch 11/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.9881 - acc: 0.7260 - val_loss: 2.5829 - val_acc: 0.4640\n",
      "Epoch 12/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.8362 - acc: 0.7752 - val_loss: 2.7353 - val_acc: 0.4597\n",
      "Epoch 13/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.6867 - acc: 0.8010 - val_loss: 2.7628 - val_acc: 0.4705\n",
      "Epoch 14/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.6247 - acc: 0.8173 - val_loss: 2.6271 - val_acc: 0.4748\n",
      "Epoch 15/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.5603 - acc: 0.8369 - val_loss: 2.8995 - val_acc: 0.4554\n",
      "Epoch 16/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.5071 - acc: 0.8497 - val_loss: 2.7391 - val_acc: 0.4812\n",
      "Epoch 17/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.4424 - acc: 0.8656 - val_loss: 3.1591 - val_acc: 0.4769\n",
      "Epoch 18/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.3946 - acc: 0.8827 - val_loss: 3.1113 - val_acc: 0.4705\n",
      "Epoch 19/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.3582 - acc: 0.8946 - val_loss: 3.4468 - val_acc: 0.4715\n",
      "Epoch 20/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.3822 - acc: 0.8921 - val_loss: 2.9603 - val_acc: 0.4909\n",
      "Epoch 21/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.3180 - acc: 0.9027 - val_loss: 3.2361 - val_acc: 0.4748\n",
      "Epoch 22/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.3161 - acc: 0.9075 - val_loss: 2.9507 - val_acc: 0.4876\n",
      "Epoch 23/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.2774 - acc: 0.9142 - val_loss: 3.1375 - val_acc: 0.4866\n",
      "Epoch 24/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.2631 - acc: 0.9208 - val_loss: 3.2181 - val_acc: 0.4844\n",
      "Epoch 25/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.2444 - acc: 0.9273 - val_loss: 3.1839 - val_acc: 0.4876\n",
      "Epoch 26/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.2654 - acc: 0.9245 - val_loss: 3.5562 - val_acc: 0.4662\n",
      "Epoch 27/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.2361 - acc: 0.9280 - val_loss: 3.2829 - val_acc: 0.4855\n",
      "Epoch 28/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.2294 - acc: 0.9353 - val_loss: 3.3389 - val_acc: 0.4919\n",
      "Epoch 29/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.2021 - acc: 0.9427 - val_loss: 3.6552 - val_acc: 0.4769\n",
      "Epoch 30/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1951 - acc: 0.9402 - val_loss: 3.4373 - val_acc: 0.4844\n",
      "Epoch 31/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.2086 - acc: 0.9395 - val_loss: 3.4269 - val_acc: 0.4855\n",
      "Epoch 32/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.2019 - acc: 0.9450 - val_loss: 3.8218 - val_acc: 0.4629\n",
      "Epoch 33/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1933 - acc: 0.9425 - val_loss: 3.7010 - val_acc: 0.4812\n",
      "Epoch 34/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1831 - acc: 0.9466 - val_loss: 3.3949 - val_acc: 0.4844\n",
      "Epoch 35/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1593 - acc: 0.9519 - val_loss: 3.5155 - val_acc: 0.4780\n",
      "Epoch 36/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1774 - acc: 0.9478 - val_loss: 3.5063 - val_acc: 0.4791\n",
      "Epoch 37/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1671 - acc: 0.9494 - val_loss: 3.6396 - val_acc: 0.4876\n",
      "Epoch 38/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1606 - acc: 0.9521 - val_loss: 3.6855 - val_acc: 0.4737\n",
      "Epoch 39/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1553 - acc: 0.9549 - val_loss: 3.4667 - val_acc: 0.4930\n",
      "Epoch 40/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1584 - acc: 0.9549 - val_loss: 3.4255 - val_acc: 0.4876\n",
      "Epoch 41/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1594 - acc: 0.9561 - val_loss: 3.6605 - val_acc: 0.5027\n",
      "Epoch 42/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1334 - acc: 0.9572 - val_loss: 3.7219 - val_acc: 0.4995\n",
      "Epoch 43/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1387 - acc: 0.9563 - val_loss: 3.7390 - val_acc: 0.4898\n",
      "Epoch 44/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1465 - acc: 0.9616 - val_loss: 3.6331 - val_acc: 0.4791\n",
      "Epoch 45/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1174 - acc: 0.9634 - val_loss: 3.5772 - val_acc: 0.4769\n",
      "Epoch 46/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1437 - acc: 0.9574 - val_loss: 3.7013 - val_acc: 0.4844\n",
      "Epoch 47/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1287 - acc: 0.9627 - val_loss: 3.6261 - val_acc: 0.4941\n",
      "Epoch 48/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1440 - acc: 0.9565 - val_loss: 3.5594 - val_acc: 0.4887\n",
      "Epoch 49/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1124 - acc: 0.9682 - val_loss: 3.6117 - val_acc: 0.4887\n",
      "Epoch 50/50\n",
      "4346/4346 [==============================] - 38s - loss: 0.1319 - acc: 0.9600 - val_loss: 3.6905 - val_acc: 0.4962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2858c161d0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=64,\n",
    "                    epochs=50,\n",
    "                    validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's plot the loss function the loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 50 epochs, we've got a final validation accuracy of around 50%. We've held out a test set for final evaluation which we do in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test loss:', 3.7721598874857496)\n",
      "('Test accuracy:', 0.49463519313304721)\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we've achieved a top-1 accuracy of around 50%. That's not too bad, considering that if we were to use a baseline strategy of taking random guesses, we would have only gotten around 1% accuracy. \n",
    "\n",
    "Now we can move on to a better strategy for training an image classifier on our small dataset: by starting with a larger and already trained network, previously \n",
    "\n",
    "To start, we will load the [VGG16 network](https://arxiv.org/pdf/1409.1556.pdf) from keras, which was trained on ImageNet and the weights saved online. If this is your first time loading VGG16, you'll need to wait a bit for the weights to download from the internet. Once the network is loaded, we can inspect the layers with the `summary()` method again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544.0\n",
      "Trainable params: 138,357,544.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vgg = keras.applications.VGG16(weights='imagenet', include_top=True)\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Notice that VGG16 is much bigger than the network we constructed. It contains 13 convolutional layers and two fully connected layers at the end, and has over 138 million parameters, around 100 times more parameters than the network we made above. Like our first network, a large proportion of the weights are stored in the connections leading into the first fully-connected layer.\n",
    "\n",
    "VGG16 was made to solve ImageNet, and achieves a [8.8% top-5 error rate](https://github.com/jcjohnson/cnn-benchmarks), which means that 91.2% of test samples were classified correctly within the top 5 predictions for each image. It's top-1 accuracy--equivalent to the accuracy we've been using (the top prediction is correct)--is 73%. This is especially impressive since there are 1000 classes, meaning that random guesses would get us only 0.1% accuracy.\n",
    "\n",
    "In order to use this network for our task, we remove the final classification layer, the 1000-neuron softmax layer at the end, which corresponds to ImageNet, and instead replace it with a new softmax layer for our dataset, which contains 97 neurons in the case of the 101_ObjectCategories dataset. This can be done in the following way, by using the keras `Model` class to initialize a new model whose input layer is the same as VGG but whose output layer is our new softmax layer, called `new_classification_layer`. Note: although it appears we are duplicating this large network, internally it is actually just copying all the layers by reference, and thus we don't need to worry about overloading the memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_classification_layer = Dense(num_classes, activation='softmax')\n",
    "inp = vgg.input\n",
    "out = new_classification_layer(vgg.layers[-2].output)\n",
    "model_new = Model(inp, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to retrain this network on the new classifications. But first, we need to freeze the weights and biases in all the layers in the network, except our new one at the end. The point is that the features that were learned in VGG should still be fairly relevant to the new image classification task. Not perfectly optimal, but most likely better than what we can find in our limited dataset. By setting the `trainable` flag in each layer false (except our new classification layer), we ensure all the weights and biases in those layers remain fixed, and we simply \"fine-tune\" the parameters in the one layer at the end. \n",
    "\n",
    "Note that sometimes you don't have to freeze all the pre-classification layers. If you have enough data, it may be desirable to retrain more than one layer at the end. In our case, we don't have enough samples for this to be worth, so we simply go ahead with the plan.\n",
    "\n",
    "After freezing the other layers, we compile the model with exactly the same optimizer and loss function as in our first network, for the sake of a fair comparison. We then run `summary` again to look at the network's architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 97)                397409    \n",
      "=================================================================\n",
      "Total params: 134,657,953.0\n",
      "Trainable params: 397,409.0\n",
      "Non-trainable params: 134,260,544.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "for l, layer in enumerate(model_new.layers[:-1]):\n",
    "    layer.trainable = False\n",
    "for l, layer in enumerate(model_new.layers[-1:]):\n",
    "    layer.trainable = True\n",
    "    \n",
    "model_new.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_new.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the summary, we see the network is identical to the vgg model we instantiated earlier, except the last layer, formerly a 1000-neuron softmax, has been replaced by a new 97-neuron softmax. Additionally, we still have roughly 134 million weights, but now the vast majority of them are \"non-trainable params\" because we froze the layers they are contained in. We now only have 397,000 trainable parameters, which is actually only a fourth of the number of parameters needed to train the first model.\n",
    "\n",
    "As before, we go ahead and train the new model, using the same hyperparametrs (batch size and number of epochs) as before, along with the same optimization algorithm. We also keep track of its history as we go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4346 samples, validate on 931 samples\n",
      "Epoch 1/50\n",
      "4346/4346 [==============================] - 41s - loss: 4.2950 - acc: 0.1666 - val_loss: 3.2372 - val_acc: 0.2889\n",
      "Epoch 2/50\n",
      "4346/4346 [==============================] - 41s - loss: 2.4873 - acc: 0.4038 - val_loss: 2.6528 - val_acc: 0.3706\n",
      "Epoch 3/50\n",
      "4346/4346 [==============================] - 41s - loss: 1.9645 - acc: 0.5124 - val_loss: 2.1180 - val_acc: 0.5070\n",
      "Epoch 4/50\n",
      "4346/4346 [==============================] - 41s - loss: 1.6791 - acc: 0.5854 - val_loss: 1.9599 - val_acc: 0.5596\n",
      "Epoch 5/50\n",
      "4346/4346 [==============================] - 41s - loss: 1.5039 - acc: 0.6233 - val_loss: 1.8346 - val_acc: 0.5822\n",
      "Epoch 6/50\n",
      "4346/4346 [==============================] - 41s - loss: 1.3700 - acc: 0.6622 - val_loss: 1.7603 - val_acc: 0.5951\n",
      "Epoch 7/50\n",
      "4346/4346 [==============================] - 41s - loss: 1.2620 - acc: 0.6845 - val_loss: 1.9753 - val_acc: 0.5682\n",
      "Epoch 8/50\n",
      "4346/4346 [==============================] - 41s - loss: 1.1866 - acc: 0.7092 - val_loss: 1.6111 - val_acc: 0.6541\n",
      "Epoch 9/50\n",
      "4346/4346 [==============================] - 41s - loss: 1.1214 - acc: 0.7246 - val_loss: 1.8558 - val_acc: 0.6004\n",
      "Epoch 10/50\n",
      "4346/4346 [==============================] - 41s - loss: 1.0554 - acc: 0.7347 - val_loss: 1.7674 - val_acc: 0.6595\n",
      "Epoch 11/50\n",
      "4346/4346 [==============================] - 41s - loss: 1.0196 - acc: 0.7517 - val_loss: 1.5718 - val_acc: 0.6692\n",
      "Epoch 12/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.9652 - acc: 0.7572 - val_loss: 1.5769 - val_acc: 0.6788\n",
      "Epoch 13/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.9204 - acc: 0.7674 - val_loss: 1.5586 - val_acc: 0.6821\n",
      "Epoch 14/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.8922 - acc: 0.7782 - val_loss: 1.5395 - val_acc: 0.6864\n",
      "Epoch 15/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.8671 - acc: 0.7812 - val_loss: 1.5532 - val_acc: 0.6767\n",
      "Epoch 16/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.8110 - acc: 0.7984 - val_loss: 1.7506 - val_acc: 0.6606\n",
      "Epoch 17/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.7502 - acc: 0.7973 - val_loss: 1.3750 - val_acc: 0.6950\n",
      "Epoch 18/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.6993 - acc: 0.8079 - val_loss: 1.3258 - val_acc: 0.6992\n",
      "Epoch 19/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.6356 - acc: 0.8233 - val_loss: 1.5802 - val_acc: 0.6864\n",
      "Epoch 20/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.6113 - acc: 0.8311 - val_loss: 1.2856 - val_acc: 0.7240\n",
      "Epoch 21/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.5881 - acc: 0.8313 - val_loss: 1.2780 - val_acc: 0.7111\n",
      "Epoch 22/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.5610 - acc: 0.8445 - val_loss: 1.4534 - val_acc: 0.7078\n",
      "Epoch 23/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.5492 - acc: 0.8465 - val_loss: 1.2304 - val_acc: 0.7368\n",
      "Epoch 24/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.5191 - acc: 0.8507 - val_loss: 1.4258 - val_acc: 0.7003\n",
      "Epoch 25/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.4948 - acc: 0.8566 - val_loss: 1.2781 - val_acc: 0.7315\n",
      "Epoch 26/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.4915 - acc: 0.8541 - val_loss: 1.3543 - val_acc: 0.7207\n",
      "Epoch 27/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.4548 - acc: 0.8698 - val_loss: 1.6429 - val_acc: 0.6853\n",
      "Epoch 28/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.4500 - acc: 0.8737 - val_loss: 1.4489 - val_acc: 0.7100\n",
      "Epoch 29/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.4393 - acc: 0.8718 - val_loss: 1.4587 - val_acc: 0.7154\n",
      "Epoch 30/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.4096 - acc: 0.8806 - val_loss: 1.5926 - val_acc: 0.7003\n",
      "Epoch 31/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.4239 - acc: 0.8748 - val_loss: 1.5323 - val_acc: 0.7164\n",
      "Epoch 32/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3998 - acc: 0.8836 - val_loss: 1.3204 - val_acc: 0.7304\n",
      "Epoch 33/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3850 - acc: 0.8852 - val_loss: 1.3457 - val_acc: 0.7368\n",
      "Epoch 34/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3688 - acc: 0.8907 - val_loss: 1.2683 - val_acc: 0.7487\n",
      "Epoch 35/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3642 - acc: 0.8898 - val_loss: 1.5154 - val_acc: 0.7100\n",
      "Epoch 36/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3704 - acc: 0.8900 - val_loss: 1.4350 - val_acc: 0.7358\n",
      "Epoch 37/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3382 - acc: 0.8992 - val_loss: 1.3683 - val_acc: 0.7573\n",
      "Epoch 38/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3256 - acc: 0.9064 - val_loss: 1.2854 - val_acc: 0.7272\n",
      "Epoch 39/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3249 - acc: 0.9050 - val_loss: 1.3636 - val_acc: 0.7304\n",
      "Epoch 40/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3172 - acc: 0.9011 - val_loss: 1.3357 - val_acc: 0.7508\n",
      "Epoch 41/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3245 - acc: 0.9050 - val_loss: 1.2946 - val_acc: 0.7701\n",
      "Epoch 42/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.3114 - acc: 0.9040 - val_loss: 1.3681 - val_acc: 0.7325\n",
      "Epoch 43/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.2860 - acc: 0.9139 - val_loss: 1.4506 - val_acc: 0.7325\n",
      "Epoch 44/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.2762 - acc: 0.9130 - val_loss: 1.4496 - val_acc: 0.7315\n",
      "Epoch 45/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.2814 - acc: 0.9135 - val_loss: 1.5113 - val_acc: 0.7207\n",
      "Epoch 46/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.2733 - acc: 0.9158 - val_loss: 1.4696 - val_acc: 0.7368\n",
      "Epoch 47/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.2576 - acc: 0.9236 - val_loss: 1.3230 - val_acc: 0.7508\n",
      "Epoch 48/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.2609 - acc: 0.9204 - val_loss: 1.2585 - val_acc: 0.7562\n",
      "Epoch 49/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.2470 - acc: 0.9225 - val_loss: 1.4517 - val_acc: 0.7422\n",
      "Epoch 50/50\n",
      "4346/4346 [==============================] - 41s - loss: 0.2372 - acc: 0.9254 - val_loss: 1.3852 - val_acc: 0.7497\n"
     ]
    }
   ],
   "source": [
    "history = model_new.fit(x_train, y_train, \n",
    "                        batch_size=16, \n",
    "                        epochs=50, \n",
    "                        validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.2949959259673962, 2.4873292677642782, 1.9644532456575754, 1.6791490945627323, 1.5039151877891728, 1.3699527028185181, 1.2620284194448166, 1.1865798278552731, 1.1214212294388519, 1.0554262819895808, 1.0195639578911, 0.96521879693521528, 0.92043418778398867, 0.8921702675593377, 0.86708424102516701, 0.81102828983880937, 0.75015084647770258, 0.69931884568734926, 0.63562792278157643, 0.61131660624039075, 0.58806850390531917, 0.5609743296894345, 0.54923666146182271, 0.51905008086920112, 0.49481008632483153, 0.4915171367983307, 0.45482381573911229, 0.45004009403154643, 0.43933838564736249, 0.40959554736138598, 0.4239096541936585, 0.3998301382842176, 0.38497586167399039, 0.3687508155536498, 0.36423922145505189, 0.37044950948708405, 0.33821546537095526, 0.32558506235413448, 0.32490420896513666, 0.31721067352214416, 0.32447247091274461, 0.31139463429530811, 0.28598313295834016, 0.27618101911231002, 0.2814358420450595, 0.27332117898904862, 0.25761526321359679, 0.2609408490644366, 0.24704756533538041, 0.23719737031426344]\n",
      "[0.16658996780018515, 0.40381960426120783, 0.5124252186466679, 0.58536585368596628, 0.62333179941058869, 0.66221813161527843, 0.68453750577984562, 0.70915784635030343, 0.72457432118748055, 0.73469857337339872, 0.75172572483184752, 0.75724804415112534, 0.76737229641933236, 0.77818683852701764, 0.78117809474495659, 0.79843534289885376, 0.79728485969590857, 0.80786930513557509, 0.82328578000018193, 0.83110906580763921, 0.83133916239336891, 0.84445467093437432, 0.84652554067224606, 0.85066728020284887, 0.85664979285816412, 0.85411872989397364, 0.86976530148145847, 0.87367694426175369, 0.87183617113704126, 0.88057984353428442, 0.87482742754698783, 0.8835710998070827, 0.88518177629120598, 0.89070409566534314, 0.8897837091304166, 0.89001380582586498, 0.89921767142199727, 0.90635066725282809, 0.90497008740929374, 0.90105844454670958, 0.90497008738186413, 0.90404970090179693, 0.91394385641969622, 0.91302346980248072, 0.91348366311108853, 0.91578462948954931, 0.92360791532443631, 0.92038656230133031, 0.92245743212149101, 0.92544868839428929]\n",
      "[3.2371561073975199, 2.6527612350168854, 2.1180493872608714, 1.9598611243685384, 1.8346011600228052, 1.7602987115288395, 1.9752821978641759, 1.6110515031087282, 1.8558396642625523, 1.7673728519812562, 1.5718134555857624, 1.5769360138445485, 1.5586458769827984, 1.5395411359252273, 1.5532159613231327, 1.7505559985286823, 1.3750097170296085, 1.3258121203627162, 1.5802108387749894, 1.285590703198016, 1.2780062719523331, 1.4534319463785172, 1.2304123403178377, 1.4258462929443949, 1.278137340645529, 1.3543464374337366, 1.6429423322995163, 1.4488755404372988, 1.4586981646356469, 1.592635652304464, 1.532256516170809, 1.3203662382288728, 1.345703237422939, 1.2683205604553223, 1.5154438381164337, 1.4350172717896492, 1.3683392680485189, 1.2854269514486563, 1.3636342953374383, 1.3356739430859381, 1.2945950339507275, 1.3680669941112127, 1.4506232881904544, 1.4496270150874535, 1.5113018760107513, 1.4696441498273938, 1.3229793695578898, 1.2584777439831407, 1.4517197039150398, 1.3852376896151763]\n",
      "[0.28893662731450304, 0.37056928037572751, 0.50698174012846897, 0.55961331907583745, 0.58216971002126994, 0.595059076326106, 0.56820622989237624, 0.65413533837787574, 0.60042964557443856, 0.65950590769023054, 0.66917293236283815, 0.67883995709946798, 0.68206229863566303, 0.68635875409194913, 0.67669172938733047, 0.66058002154629925, 0.69495166494049909, 0.69924812036477402, 0.68635875409194913, 0.7239527390543552, 0.7110633727815302, 0.70784103121332398, 0.73684210532718009, 0.70032223422084283, 0.73147153604683635, 0.72073039745413781, 0.68528464023588043, 0.7099892589254615, 0.71535982820580524, 0.70032223422084283, 0.71643394206187394, 0.73039742219076764, 0.73684210532718009, 0.74865735774393627, 0.7099892589254615, 0.73576799147111127, 0.75725026859248623, 0.72717508062256142, 0.73039742219076764, 0.75080558545607379, 0.77013963486531112, 0.73254564990290505, 0.73254564987089399, 0.73147153604683635, 0.72073039745413781, 0.73684210532718009, 0.75080558545607379, 0.75617615473641742, 0.74221267460752383, 0.74973147160000497]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(history.history['loss'])\n",
    "print(history.history['acc'])\n",
    "\n",
    "print(history.history['val_loss'])\n",
    "print(history.history['val_acc'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test loss:', 1.4147466119266887)\n",
      "('Test accuracy:', 0.75214592274678116)\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n",
    "# with rmsprop - 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4346 samples, validate on 931 samples\n",
      "Epoch 1/100\n",
      "4346/4346 [==============================] - 41s - loss: 3.7791 - acc: 0.2897 - val_loss: 2.5364 - val_acc: 0.4415\n",
      "Epoch 2/100\n",
      "4346/4346 [==============================] - 41s - loss: 2.0970 - acc: 0.5306 - val_loss: 2.0981 - val_acc: 0.5220\n",
      "Epoch 3/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.6259 - acc: 0.6183 - val_loss: 1.6269 - val_acc: 0.5994\n",
      "Epoch 4/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.2606 - acc: 0.6758 - val_loss: 1.5327 - val_acc: 0.6359\n",
      "Epoch 5/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1240 - acc: 0.7057 - val_loss: 1.5208 - val_acc: 0.6273\n",
      "Epoch 6/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.0094 - acc: 0.7331 - val_loss: 1.5390 - val_acc: 0.6273\n",
      "Epoch 7/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.8922 - acc: 0.7589 - val_loss: 1.3389 - val_acc: 0.6541\n",
      "Epoch 8/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.8547 - acc: 0.7697 - val_loss: 1.4524 - val_acc: 0.6531\n",
      "Epoch 9/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.7617 - acc: 0.7936 - val_loss: 1.3116 - val_acc: 0.6638\n",
      "Epoch 10/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.7105 - acc: 0.8000 - val_loss: 1.4214 - val_acc: 0.6810\n",
      "Epoch 11/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6854 - acc: 0.8118 - val_loss: 1.3549 - val_acc: 0.6831\n",
      "Epoch 12/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6458 - acc: 0.8205 - val_loss: 1.4407 - val_acc: 0.6960\n",
      "Epoch 13/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6596 - acc: 0.8267 - val_loss: 1.5424 - val_acc: 0.6831\n",
      "Epoch 14/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6968 - acc: 0.8198 - val_loss: 1.6062 - val_acc: 0.6756\n",
      "Epoch 15/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6721 - acc: 0.8346 - val_loss: 1.3661 - val_acc: 0.7003\n",
      "Epoch 16/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6410 - acc: 0.8304 - val_loss: 1.3390 - val_acc: 0.6928\n",
      "Epoch 17/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6077 - acc: 0.8378 - val_loss: 1.3179 - val_acc: 0.7003\n",
      "Epoch 18/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4995 - acc: 0.8691 - val_loss: 1.4240 - val_acc: 0.6971\n",
      "Epoch 19/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4852 - acc: 0.8769 - val_loss: 1.2983 - val_acc: 0.7100\n",
      "Epoch 20/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5216 - acc: 0.8555 - val_loss: 1.5760 - val_acc: 0.7068\n",
      "Epoch 21/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4962 - acc: 0.8780 - val_loss: 1.4584 - val_acc: 0.7014\n",
      "Epoch 22/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5023 - acc: 0.8728 - val_loss: 1.1956 - val_acc: 0.7368\n",
      "Epoch 23/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4446 - acc: 0.8868 - val_loss: 1.2494 - val_acc: 0.7197\n",
      "Epoch 24/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4502 - acc: 0.8905 - val_loss: 1.3811 - val_acc: 0.7293\n",
      "Epoch 25/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4378 - acc: 0.8932 - val_loss: 1.4760 - val_acc: 0.7003\n",
      "Epoch 26/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4523 - acc: 0.8794 - val_loss: 1.3435 - val_acc: 0.7304\n",
      "Epoch 27/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4335 - acc: 0.8854 - val_loss: 1.5394 - val_acc: 0.7025\n",
      "Epoch 28/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.3909 - acc: 0.9054 - val_loss: 1.5505 - val_acc: 0.7046\n",
      "Epoch 29/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.3609 - acc: 0.9133 - val_loss: 1.3305 - val_acc: 0.7197\n",
      "Epoch 30/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4293 - acc: 0.8978 - val_loss: 1.4251 - val_acc: 0.7068\n",
      "Epoch 31/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.3755 - acc: 0.9047 - val_loss: 1.3992 - val_acc: 0.7154\n",
      "Epoch 32/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4939 - acc: 0.8891 - val_loss: 1.5142 - val_acc: 0.7111\n",
      "Epoch 33/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.3269 - acc: 0.9222 - val_loss: 1.5123 - val_acc: 0.7272\n",
      "Epoch 34/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.3631 - acc: 0.9096 - val_loss: 1.5379 - val_acc: 0.7111\n",
      "Epoch 35/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.3318 - acc: 0.9220 - val_loss: 1.2897 - val_acc: 0.7422\n",
      "Epoch 36/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4209 - acc: 0.9105 - val_loss: 1.6931 - val_acc: 0.7078\n",
      "Epoch 37/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6342 - acc: 0.9054 - val_loss: 1.6462 - val_acc: 0.7422\n",
      "Epoch 38/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.7157 - acc: 0.8923 - val_loss: 1.5845 - val_acc: 0.7336\n",
      "Epoch 39/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6936 - acc: 0.9034 - val_loss: 1.7386 - val_acc: 0.7218\n",
      "Epoch 40/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.7467 - acc: 0.8935 - val_loss: 1.7699 - val_acc: 0.7336\n",
      "Epoch 41/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.7004 - acc: 0.9142 - val_loss: 1.7163 - val_acc: 0.7100\n",
      "Epoch 42/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6889 - acc: 0.8983 - val_loss: 1.6528 - val_acc: 0.7325\n",
      "Epoch 43/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5655 - acc: 0.9185 - val_loss: 1.6041 - val_acc: 0.7207\n",
      "Epoch 44/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6056 - acc: 0.9144 - val_loss: 1.9996 - val_acc: 0.7046\n",
      "Epoch 45/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6566 - acc: 0.8948 - val_loss: 1.7804 - val_acc: 0.7465\n",
      "Epoch 46/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5461 - acc: 0.9314 - val_loss: 1.6860 - val_acc: 0.7444\n",
      "Epoch 47/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5191 - acc: 0.9298 - val_loss: 1.6360 - val_acc: 0.7282\n",
      "Epoch 48/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5483 - acc: 0.9261 - val_loss: 1.8491 - val_acc: 0.7154\n",
      "Epoch 49/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6003 - acc: 0.9213 - val_loss: 2.0162 - val_acc: 0.6853\n",
      "Epoch 50/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5297 - acc: 0.9195 - val_loss: 1.9901 - val_acc: 0.6992\n",
      "Epoch 51/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4588 - acc: 0.9266 - val_loss: 1.8329 - val_acc: 0.7121\n",
      "Epoch 52/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4262 - acc: 0.9374 - val_loss: 1.9779 - val_acc: 0.6896\n",
      "Epoch 53/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5269 - acc: 0.9144 - val_loss: 1.8570 - val_acc: 0.7111\n",
      "Epoch 54/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4283 - acc: 0.9365 - val_loss: 1.7119 - val_acc: 0.7218\n",
      "Epoch 55/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.3875 - acc: 0.9418 - val_loss: 1.5504 - val_acc: 0.7573\n",
      "Epoch 56/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.3785 - acc: 0.9531 - val_loss: 1.5207 - val_acc: 0.7530\n",
      "Epoch 57/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4762 - acc: 0.9436 - val_loss: 1.6699 - val_acc: 0.7487\n",
      "Epoch 58/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5862 - acc: 0.9195 - val_loss: 2.2415 - val_acc: 0.6907\n",
      "Epoch 59/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6747 - acc: 0.9100 - val_loss: 1.9377 - val_acc: 0.7207\n",
      "Epoch 60/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.6142 - acc: 0.9243 - val_loss: 2.0080 - val_acc: 0.7186\n",
      "Epoch 61/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5642 - acc: 0.9381 - val_loss: 2.1261 - val_acc: 0.7111\n",
      "Epoch 62/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4822 - acc: 0.9468 - val_loss: 1.7546 - val_acc: 0.7401\n",
      "Epoch 63/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5903 - acc: 0.9179 - val_loss: 1.8176 - val_acc: 0.7218\n",
      "Epoch 64/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5965 - acc: 0.9172 - val_loss: 2.0585 - val_acc: 0.7035\n",
      "Epoch 65/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.4868 - acc: 0.9475 - val_loss: 1.6904 - val_acc: 0.7454\n",
      "Epoch 66/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.5536 - acc: 0.9411 - val_loss: 1.7307 - val_acc: 0.7583\n",
      "Epoch 67/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.7044 - acc: 0.9202 - val_loss: 1.9952 - val_acc: 0.7197\n",
      "Epoch 68/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.9551 - acc: 0.9128 - val_loss: 2.6185 - val_acc: 0.6627\n",
      "Epoch 69/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.7024 - acc: 0.9218 - val_loss: 1.8489 - val_acc: 0.7422\n",
      "Epoch 70/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.7150 - acc: 0.9289 - val_loss: 2.2893 - val_acc: 0.6971\n",
      "Epoch 71/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.9113 - acc: 0.9054 - val_loss: 2.1397 - val_acc: 0.7336\n",
      "Epoch 72/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.8426 - acc: 0.9202 - val_loss: 2.2757 - val_acc: 0.7143\n",
      "Epoch 73/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.0167 - acc: 0.9038 - val_loss: 2.0627 - val_acc: 0.7411\n",
      "Epoch 74/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.8614 - acc: 0.9317 - val_loss: 2.2467 - val_acc: 0.7164\n",
      "Epoch 75/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.9589 - acc: 0.9144 - val_loss: 2.1078 - val_acc: 0.7347\n",
      "Epoch 76/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.2273 - acc: 0.8861 - val_loss: 2.2886 - val_acc: 0.7218\n",
      "Epoch 77/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.0544 - acc: 0.9077 - val_loss: 2.3381 - val_acc: 0.7025\n",
      "Epoch 78/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.0215 - acc: 0.9123 - val_loss: 2.0725 - val_acc: 0.7487\n",
      "Epoch 79/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.0103 - acc: 0.9181 - val_loss: 2.2183 - val_acc: 0.7261\n",
      "Epoch 80/100\n",
      "4346/4346 [==============================] - 41s - loss: 0.9540 - acc: 0.9282 - val_loss: 2.2849 - val_acc: 0.7229\n",
      "Epoch 81/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.0275 - acc: 0.9070 - val_loss: 2.2843 - val_acc: 0.7293\n",
      "Epoch 82/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.3171 - acc: 0.8790 - val_loss: 2.9309 - val_acc: 0.6692\n",
      "Epoch 83/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.2575 - acc: 0.8962 - val_loss: 2.3617 - val_acc: 0.7272\n",
      "Epoch 84/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1727 - acc: 0.9008 - val_loss: 2.4457 - val_acc: 0.7143\n",
      "Epoch 85/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1943 - acc: 0.8930 - val_loss: 2.6376 - val_acc: 0.7143\n",
      "Epoch 86/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.2229 - acc: 0.8978 - val_loss: 2.7259 - val_acc: 0.6778\n",
      "Epoch 87/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1818 - acc: 0.8981 - val_loss: 2.2443 - val_acc: 0.7368\n",
      "Epoch 88/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.0461 - acc: 0.9218 - val_loss: 2.3781 - val_acc: 0.7164\n",
      "Epoch 89/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1483 - acc: 0.8988 - val_loss: 2.7183 - val_acc: 0.7035\n",
      "Epoch 90/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.0533 - acc: 0.9181 - val_loss: 2.3432 - val_acc: 0.7272\n",
      "Epoch 91/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.0843 - acc: 0.9066 - val_loss: 2.4401 - val_acc: 0.7197\n",
      "Epoch 92/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1187 - acc: 0.9047 - val_loss: 2.3974 - val_acc: 0.7207\n",
      "Epoch 93/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1120 - acc: 0.9070 - val_loss: 2.5963 - val_acc: 0.7014\n",
      "Epoch 94/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.2876 - acc: 0.8873 - val_loss: 2.6452 - val_acc: 0.6971\n",
      "Epoch 95/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1696 - acc: 0.9093 - val_loss: 2.4183 - val_acc: 0.7197\n",
      "Epoch 96/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1175 - acc: 0.9275 - val_loss: 2.1629 - val_acc: 0.7530\n",
      "Epoch 97/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.0693 - acc: 0.9231 - val_loss: 2.5041 - val_acc: 0.7143\n",
      "Epoch 98/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1815 - acc: 0.8907 - val_loss: 2.5615 - val_acc: 0.7154\n",
      "Epoch 99/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1345 - acc: 0.9029 - val_loss: 2.6036 - val_acc: 0.6907\n",
      "Epoch 100/100\n",
      "4346/4346 [==============================] - 41s - loss: 1.1147 - acc: 0.9135 - val_loss: 2.4774 - val_acc: 0.7068\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# with adam - 16\n",
    "history = model_new.fit(x_train, y_train, \n",
    "                        batch_size=16, \n",
    "                        epochs=100, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.7790937493925978, 2.0970398478391692, 1.6259035915479207, 1.2606343830778366, 1.1240255247871831, 1.0093628710938993, 0.89224485450223245, 0.85471659687549573, 0.76169356513122122, 0.71047449165426313, 0.68537774897367787, 0.64579114655278769, 0.65955451748034288, 0.6968140838624034, 0.67212362732143371, 0.64097165518294152, 0.60772312134572892, 0.49952661492242495, 0.48521878651578582, 0.5216190933056597, 0.49623357044473748, 0.50231457658341772, 0.4446272985539127, 0.45016871240492301, 0.43783838417787352, 0.45232719857223397, 0.43351669246017988, 0.39091518754396698, 0.36086868335828493, 0.42931456646842997, 0.37546639811304161, 0.49386546966005623, 0.32687784339786879, 0.36310052895640765, 0.33182020846701166, 0.42094313476647832, 0.6341937119822677, 0.71568800993195147, 0.6935976320115641, 0.74672852675578338, 0.70043119345223759, 0.68886478043521771, 0.56551276913853055, 0.60562490715931983, 0.65656631367835672, 0.54612868000764792, 0.51909095522194582, 0.54829271179158878, 0.60030375048303886, 0.52969502742919561, 0.45878610986225338, 0.42618125336595286, 0.5268898602550206, 0.42834475067713301, 0.38748849098031907, 0.37854222209145455, 0.47615382354749386, 0.58622006823047756, 0.67469476094513148, 0.61418092074159836, 0.56420586957575669, 0.48221909958820786, 0.59029946653999554, 0.59652396537643393, 0.48675721378160042, 0.55355051933219668, 0.70443621761299713, 0.95514374325271345, 0.70243997540919811, 0.71501111730898259, 0.91126787384845109, 0.84260519456505789, 1.0166866471035727, 0.8614024548450675, 0.95891442229627133, 1.2272969846635349, 1.0544378744551652, 1.0214787255263578, 1.0102592756156541, 0.95403283401197392, 1.0275365558037013, 1.3170815940477678, 1.2575469885998201, 1.172688398207353, 1.194315467697765, 1.2228532317651268, 1.1817813991675648, 1.0460879802142407, 1.1483115149283991, 1.0532726645668959, 1.0842768638719964, 1.1186574583729201, 1.1119731071783101, 1.2876061563865653, 1.1695684096423435, 1.1174540808804725, 1.0692631494912508, 1.1814539820500187, 1.1345014031670557, 1.1146963018901712]\n",
      "[0.28969167055646999, 0.53060285314348399, 0.61826967331762972, 0.67579383341003219, 0.70570639663174906, 0.73308789686184572, 0.758858720607819, 0.76967326282522286, 0.79360331336419487, 0.80004601932811781, 0.81178094797072953, 0.82052462028568374, 0.82673722966387697, 0.81983433041877585, 0.83456051544390453, 0.83041877585844237, 0.83778186832986223, 0.86907501150483202, 0.8768982973122893, 0.85549930965521892, 0.87804878043294554, 0.87275655769939753, 0.88679245285761832, 0.8904739990247541, 0.89323515876668202, 0.87942936027647989, 0.88541187295922474, 0.90543028071790155, 0.91325356644306976, 0.89783709155103331, 0.90473999082356404, 0.88909341923607921, 0.92222733548090197, 0.90957202019364503, 0.92199723878545359, 0.91049240675600129, 0.90543028069047182, 0.89231477214946653, 0.90335941089774074, 0.8934652554621304, 0.91417395306028537, 0.89829728480478177, 0.91854578923147723, 0.91440404970087441, 0.89484583519594607, 0.93143120110446387, 0.92982052462034059, 0.92613897837091574, 0.92130694886368647, 0.91946617573897405, 0.92659917159723459, 0.93741371370491977, 0.91440404964601507, 0.93649332719742295, 0.94178554987611174, 0.95306028531983433, 0.94362632308311301, 0.91946617579383338, 0.91003221355711217, 0.92429820524620343, 0.93810400368154623, 0.94684767599650044, 0.91785549933713972, 0.91716520936051327, 0.94753796594569717, 0.94109526003663357, 0.92015646574303023, 0.91279337321675103, 0.92176714219972389, 0.92890013808541405, 0.90543028066304221, 0.92015646574303023, 0.90381960423377816, 0.93166129774505291, 0.91440404970087441, 0.88610216288099186, 0.90773124712379194, 0.91233317988071361, 0.91808559589543981, 0.92820984808135787, 0.90704095714716548, 0.87896916705016104, 0.89622641512176926, 0.9008283478512612, 0.89300506212609299, 0.89783709157846292, 0.89806718824648168, 0.92176714219972389, 0.8987574781682488, 0.91808559589543981, 0.90658076394827636, 0.90473999079613443, 0.90704095717459521, 0.88725264616622612, 0.90934192355305599, 0.92751955815959075, 0.92314772198839889, 0.89070409566534314, 0.90289921761656267, 0.91348366308365891]\n",
      "[2.5363849459093957, 2.0980703295493615, 1.626891088024758, 1.5326619849937431, 1.5208070521451988, 1.5389638659009104, 1.3388795570963563, 1.4523519805115352, 1.3115654027551638, 1.4213889520995471, 1.3549437348747868, 1.4406757700456072, 1.5423674478438691, 1.6062364609617936, 1.3661113941579832, 1.3389599287420286, 1.317862912228233, 1.4240062222701011, 1.2983448461866531, 1.5760086886224633, 1.4584009370793589, 1.1956399255357162, 1.2494446200535709, 1.3810646879685808, 1.4759730864416261, 1.3434508702168531, 1.5394288206715077, 1.5505387446293897, 1.3305454382194741, 1.4251482279293006, 1.3991879712626194, 1.5142056364506067, 1.5122711299953604, 1.537907445469169, 1.2896528918812278, 1.6931020703914983, 1.6462302328308467, 1.584490799942283, 1.7386383108882719, 1.7698940567504196, 1.7163210647574814, 1.6527831577460825, 1.6041203175144216, 1.999552488551104, 1.7804484615776643, 1.6860346345973194, 1.6360030694115431, 1.8490888343845864, 2.016214111053598, 1.9901424762897921, 1.8329419879088724, 1.9779194311091775, 1.8569620129373225, 1.7118524250231291, 1.5503983984188157, 1.520733828831436, 1.6699194931190073, 2.2415336159194963, 1.9377011082482261, 2.0079545549624203, 2.1260573129008589, 1.754559499236618, 1.8175783637514944, 2.0584990443015587, 1.6903968552801714, 1.7307366635950501, 1.9951839600674703, 2.6185465847511167, 1.8489355590238479, 2.2892938219771093, 2.1396962710277094, 2.2757189125302784, 2.0626607723894232, 2.2467462752738649, 2.1077887894131573, 2.288619018394888, 2.3381189856954343, 2.0725442244082082, 2.21828654706158, 2.2848966936035646, 2.2843222477254241, 2.9309427704898168, 2.3616871096265815, 2.4457188354270927, 2.6375624541949509, 2.7259027530248132, 2.2442620173176686, 2.3780665996891601, 2.7183385243605338, 2.3432255818175451, 2.4400761355390865, 2.3973823162205878, 2.5963105945402774, 2.6451710312758303, 2.4182746405248099, 2.1628986337131129, 2.5040973177785903, 2.5615125378527779, 2.6036228944100319, 2.4774237665018641]\n",
      "[0.44146079490827567, 0.52201933408142032, 0.59935553175038092, 0.63587540285671817, 0.62728249197615715, 0.62728249197615715, 0.6541353384098868, 0.6530612245538181, 0.66380236308249441, 0.68098818477959433, 0.68313641252374291, 0.6960257787965678, 0.68313641249173185, 0.67561761553126165, 0.70032223422084283, 0.69280343722836157, 0.70032223418883166, 0.69709989262062544, 0.70998925889345044, 0.70676691732524421, 0.70139634807691154, 0.73684210532718009, 0.71965628363008016, 0.72932330830268777, 0.70032223422084283, 0.73039742219076764, 0.70247046193298024, 0.70461868964511776, 0.71965628363008016, 0.70676691735725528, 0.71535982820580524, 0.7110633727815302, 0.72717508062256142, 0.7110633727815302, 0.74221267460752383, 0.70784103121332398, 0.74221267457551265, 0.73361976375897386, 0.72180451134221768, 0.73361976375897386, 0.7099892589254615, 0.73254564990290505, 0.72073039748614887, 0.70461868958109564, 0.74650913003179875, 0.74436090231966123, 0.72824919447863012, 0.71535982820580524, 0.68528464023588043, 0.69924812033276296, 0.71213748663759902, 0.68958109562814429, 0.7110633727815302, 0.72180451134221768, 0.75725026859248623, 0.75295381316821119, 0.74865735774393627, 0.690655209484213, 0.72073039745413781, 0.7185821697420004, 0.7110633727815302, 0.74006444689538631, 0.72180451134221768, 0.70354457578904905, 0.74543501617573005, 0.75832438241654387, 0.71965628363008016, 0.66272824925843676, 0.74221267460752383, 0.69709989265263661, 0.73361976375897386, 0.71428571431772536, 0.74113856075145501, 0.71643394206187394, 0.73469387761504257, 0.72180451134221768, 0.70247046193298024, 0.74865735774393627, 0.72610096676649261, 0.72287862516627532, 0.72932330830268777, 0.66917293239484921, 0.72717508062256142, 0.71428571434973642, 0.7142857142857143, 0.67776584321138811, 0.73684210529516903, 0.71643394202986288, 0.70354457575703799, 0.72717508059055025, 0.7196562835980691, 0.72073039745413781, 0.70139634804490048, 0.69709989262062544, 0.7196562835980691, 0.75295381313620013, 0.71428571431772536, 0.71535982817379407, 0.690655209484213, 0.70676691732524421]\n",
      "('Test loss:', 2.4073744966236816)\n",
      "('Test accuracy:', 0.70600858369098718)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# with adam - 16\n",
    "print(history.history['loss'])\n",
    "print(history.history['acc'])\n",
    "\n",
    "print(history.history['val_loss'])\n",
    "print(history.history['val_acc'])\n",
    "\n",
    "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 97)                397409    \n",
      "=================================================================\n",
      "Total params: 134,657,953.0\n",
      "Trainable params: 397,409.0\n",
      "Non-trainable params: 134,260,544.0\n",
      "_________________________________________________________________\n",
      "Train on 4346 samples, validate on 931 samples\n",
      "Epoch 1/100\n",
      "4346/4346 [==============================] - 47s - loss: 3.9743 - acc: 0.2936 - val_loss: 3.0671 - val_acc: 0.4103\n",
      "Epoch 2/100\n",
      "4346/4346 [==============================] - 47s - loss: 2.2760 - acc: 0.5318 - val_loss: 2.4113 - val_acc: 0.5220\n",
      "Epoch 3/100\n",
      "4346/4346 [==============================] - 47s - loss: 2.0074 - acc: 0.5934 - val_loss: 2.0624 - val_acc: 0.5542\n",
      "Epoch 4/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.8759 - acc: 0.6291 - val_loss: 2.0191 - val_acc: 0.5918\n",
      "Epoch 5/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.6815 - acc: 0.6721 - val_loss: 2.3435 - val_acc: 0.6101\n",
      "Epoch 6/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.6689 - acc: 0.6730 - val_loss: 2.3126 - val_acc: 0.6305\n",
      "Epoch 7/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5697 - acc: 0.7133 - val_loss: 2.4038 - val_acc: 0.6316\n",
      "Epoch 8/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5398 - acc: 0.7209 - val_loss: 2.5344 - val_acc: 0.6069\n",
      "Epoch 9/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4721 - acc: 0.7400 - val_loss: 2.3657 - val_acc: 0.6165\n",
      "Epoch 10/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4001 - acc: 0.7600 - val_loss: 2.1596 - val_acc: 0.6617\n",
      "Epoch 11/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3900 - acc: 0.7566 - val_loss: 2.1854 - val_acc: 0.6606\n",
      "Epoch 12/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5211 - acc: 0.7575 - val_loss: 2.3112 - val_acc: 0.6627\n",
      "Epoch 13/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2812 - acc: 0.7851 - val_loss: 2.2310 - val_acc: 0.6745\n",
      "Epoch 14/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3520 - acc: 0.7713 - val_loss: 2.2618 - val_acc: 0.6681\n",
      "Epoch 15/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3858 - acc: 0.7729 - val_loss: 2.3861 - val_acc: 0.6531\n",
      "Epoch 16/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4746 - acc: 0.7865 - val_loss: 2.2107 - val_acc: 0.6788\n",
      "Epoch 17/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2060 - acc: 0.8132 - val_loss: 2.5914 - val_acc: 0.6423\n",
      "Epoch 18/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2460 - acc: 0.8104 - val_loss: 2.4047 - val_acc: 0.6563\n",
      "Epoch 19/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2590 - acc: 0.8095 - val_loss: 2.3281 - val_acc: 0.6821\n",
      "Epoch 20/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.0993 - acc: 0.8277 - val_loss: 1.9739 - val_acc: 0.7100\n",
      "Epoch 21/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2364 - acc: 0.8208 - val_loss: 2.1496 - val_acc: 0.6950\n",
      "Epoch 22/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2537 - acc: 0.8173 - val_loss: 2.3289 - val_acc: 0.6821\n",
      "Epoch 23/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2813 - acc: 0.8182 - val_loss: 2.6497 - val_acc: 0.6713\n",
      "Epoch 24/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2089 - acc: 0.8295 - val_loss: 2.5659 - val_acc: 0.6520\n",
      "Epoch 25/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2015 - acc: 0.8339 - val_loss: 2.2750 - val_acc: 0.7025\n",
      "Epoch 26/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.1863 - acc: 0.8366 - val_loss: 2.5911 - val_acc: 0.6917\n",
      "Epoch 27/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.1868 - acc: 0.8442 - val_loss: 2.4755 - val_acc: 0.6638\n",
      "Epoch 28/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4457 - acc: 0.8254 - val_loss: 3.0402 - val_acc: 0.6208\n",
      "Epoch 29/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3240 - acc: 0.8447 - val_loss: 2.4502 - val_acc: 0.7025\n",
      "Epoch 30/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3537 - acc: 0.8401 - val_loss: 2.4874 - val_acc: 0.6756\n",
      "Epoch 31/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3256 - acc: 0.8389 - val_loss: 2.6510 - val_acc: 0.6681\n",
      "Epoch 32/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3495 - acc: 0.8396 - val_loss: 2.8598 - val_acc: 0.6660\n",
      "Epoch 33/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3789 - acc: 0.8511 - val_loss: 2.7950 - val_acc: 0.6767\n",
      "Epoch 34/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4488 - acc: 0.8403 - val_loss: 3.0731 - val_acc: 0.6520\n",
      "Epoch 35/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5648 - acc: 0.8387 - val_loss: 2.8620 - val_acc: 0.6702\n",
      "Epoch 36/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4623 - acc: 0.8329 - val_loss: 2.7466 - val_acc: 0.6821\n",
      "Epoch 37/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3601 - acc: 0.8592 - val_loss: 2.8277 - val_acc: 0.6788\n",
      "Epoch 38/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2909 - acc: 0.8636 - val_loss: 2.8190 - val_acc: 0.6702\n",
      "Epoch 39/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.6933 - acc: 0.8132 - val_loss: 2.6406 - val_acc: 0.6874\n",
      "Epoch 40/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3262 - acc: 0.8686 - val_loss: 2.6968 - val_acc: 0.6853\n",
      "Epoch 41/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3296 - acc: 0.8691 - val_loss: 2.9326 - val_acc: 0.6745\n",
      "Epoch 42/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4587 - acc: 0.8534 - val_loss: 2.6551 - val_acc: 0.6992\n",
      "Epoch 43/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3709 - acc: 0.8631 - val_loss: 2.8385 - val_acc: 0.6788\n",
      "Epoch 44/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4193 - acc: 0.8569 - val_loss: 2.8499 - val_acc: 0.6799\n",
      "Epoch 45/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5109 - acc: 0.8470 - val_loss: 2.7724 - val_acc: 0.7089\n",
      "Epoch 46/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4833 - acc: 0.8578 - val_loss: 2.5887 - val_acc: 0.7025\n",
      "Epoch 47/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5690 - acc: 0.8491 - val_loss: 3.0447 - val_acc: 0.6767\n",
      "Epoch 48/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3008 - acc: 0.8813 - val_loss: 2.4304 - val_acc: 0.7154\n",
      "Epoch 49/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4005 - acc: 0.8638 - val_loss: 3.0293 - val_acc: 0.6778\n",
      "Epoch 50/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4387 - acc: 0.8629 - val_loss: 2.6808 - val_acc: 0.6917\n",
      "Epoch 51/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3310 - acc: 0.8746 - val_loss: 2.7052 - val_acc: 0.7057\n",
      "Epoch 52/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3584 - acc: 0.8652 - val_loss: 2.6997 - val_acc: 0.6982\n",
      "Epoch 53/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4269 - acc: 0.8537 - val_loss: 3.0214 - val_acc: 0.6778\n",
      "Epoch 54/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4761 - acc: 0.8617 - val_loss: 2.8163 - val_acc: 0.7068\n",
      "Epoch 55/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2203 - acc: 0.8840 - val_loss: 2.4916 - val_acc: 0.7186\n",
      "Epoch 56/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2382 - acc: 0.8813 - val_loss: 2.3775 - val_acc: 0.7401\n",
      "Epoch 57/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2723 - acc: 0.8767 - val_loss: 2.6035 - val_acc: 0.7057\n",
      "Epoch 58/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2230 - acc: 0.8785 - val_loss: 2.5053 - val_acc: 0.7186\n",
      "Epoch 59/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.1952 - acc: 0.8774 - val_loss: 2.3972 - val_acc: 0.7250\n",
      "Epoch 60/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.1339 - acc: 0.8870 - val_loss: 2.2304 - val_acc: 0.7454\n",
      "Epoch 61/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.0225 - acc: 0.9011 - val_loss: 2.7589 - val_acc: 0.6853\n",
      "Epoch 62/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.1640 - acc: 0.8790 - val_loss: 2.3364 - val_acc: 0.7401\n",
      "Epoch 63/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.0040 - acc: 0.8902 - val_loss: 2.7715 - val_acc: 0.6842\n",
      "Epoch 64/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.0094 - acc: 0.8875 - val_loss: 2.7992 - val_acc: 0.6992\n",
      "Epoch 65/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.9498 - acc: 0.9038 - val_loss: 2.9062 - val_acc: 0.6831\n",
      "Epoch 66/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.9457 - acc: 0.9043 - val_loss: 2.1586 - val_acc: 0.7411\n",
      "Epoch 67/100\n",
      "4346/4346 [==============================] - 48s - loss: 0.9788 - acc: 0.9004 - val_loss: 2.2026 - val_acc: 0.7379\n",
      "Epoch 68/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.9546 - acc: 0.9080 - val_loss: 2.4803 - val_acc: 0.7164\n",
      "Epoch 69/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.0462 - acc: 0.9008 - val_loss: 2.3721 - val_acc: 0.7336\n",
      "Epoch 70/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.1696 - acc: 0.8891 - val_loss: 2.6327 - val_acc: 0.7175\n",
      "Epoch 71/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2056 - acc: 0.8877 - val_loss: 2.8683 - val_acc: 0.6907\n",
      "Epoch 72/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2165 - acc: 0.8856 - val_loss: 2.6327 - val_acc: 0.7186\n",
      "Epoch 73/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.1702 - acc: 0.8978 - val_loss: 2.6935 - val_acc: 0.7111\n",
      "Epoch 74/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2440 - acc: 0.8859 - val_loss: 2.7886 - val_acc: 0.7089\n",
      "Epoch 75/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3622 - acc: 0.8721 - val_loss: 2.7574 - val_acc: 0.7143\n",
      "Epoch 76/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3613 - acc: 0.8935 - val_loss: 2.7216 - val_acc: 0.7175\n",
      "Epoch 77/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3539 - acc: 0.8893 - val_loss: 2.8412 - val_acc: 0.7207\n",
      "Epoch 78/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4134 - acc: 0.8748 - val_loss: 2.8711 - val_acc: 0.7250\n",
      "Epoch 79/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4972 - acc: 0.8693 - val_loss: 2.8732 - val_acc: 0.7154\n",
      "Epoch 80/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3530 - acc: 0.8909 - val_loss: 3.0319 - val_acc: 0.7035\n",
      "Epoch 81/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5026 - acc: 0.8723 - val_loss: 2.6692 - val_acc: 0.7250\n",
      "Epoch 82/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3329 - acc: 0.8877 - val_loss: 2.9899 - val_acc: 0.6939\n",
      "Epoch 83/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4465 - acc: 0.8794 - val_loss: 2.6804 - val_acc: 0.7347\n",
      "Epoch 84/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.3725 - acc: 0.8877 - val_loss: 3.0135 - val_acc: 0.7003\n",
      "Epoch 85/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5012 - acc: 0.8778 - val_loss: 2.9349 - val_acc: 0.7293\n",
      "Epoch 86/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.6574 - acc: 0.8633 - val_loss: 2.9441 - val_acc: 0.7229\n",
      "Epoch 87/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4878 - acc: 0.8843 - val_loss: 2.9114 - val_acc: 0.7164\n",
      "Epoch 88/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5179 - acc: 0.8810 - val_loss: 3.0579 - val_acc: 0.7143\n",
      "Epoch 89/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5490 - acc: 0.8808 - val_loss: 3.2395 - val_acc: 0.7014\n",
      "Epoch 90/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5004 - acc: 0.8866 - val_loss: 2.8903 - val_acc: 0.7207\n",
      "Epoch 91/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.8130 - acc: 0.8488 - val_loss: 3.4347 - val_acc: 0.6627\n",
      "Epoch 92/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5469 - acc: 0.8778 - val_loss: 2.8688 - val_acc: 0.7154\n",
      "Epoch 93/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4291 - acc: 0.8969 - val_loss: 3.1550 - val_acc: 0.7025\n",
      "Epoch 94/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.6193 - acc: 0.8705 - val_loss: 3.5723 - val_acc: 0.6724\n",
      "Epoch 95/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5260 - acc: 0.8790 - val_loss: 2.9672 - val_acc: 0.7154\n",
      "Epoch 96/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5033 - acc: 0.8859 - val_loss: 3.5408 - val_acc: 0.6756\n",
      "Epoch 97/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.6513 - acc: 0.8633 - val_loss: 3.4903 - val_acc: 0.6670\n",
      "Epoch 98/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4822 - acc: 0.8946 - val_loss: 3.0725 - val_acc: 0.7046\n",
      "Epoch 99/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5377 - acc: 0.8767 - val_loss: 3.2439 - val_acc: 0.6917\n",
      "Epoch 100/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.5216 - acc: 0.8817 - val_loss: 2.8905 - val_acc: 0.7282\n",
      "[3.9743290374863824, 2.2760124257349981, 2.0073585587605538, 1.8759255435881776, 1.6814935442027659, 1.6688617416998235, 1.5696988599713475, 1.5398380350687118, 1.4720764304751259, 1.4000654543650983, 1.3900165183343649, 1.5211309812370699, 1.2811875988466213, 1.3520124577315795, 1.3857882201819436, 1.4746482669640704, 1.2060062268001956, 1.2459792919324151, 1.2589525418183307, 1.0992858922316606, 1.2364439940851988, 1.2536721396122963, 1.2813143205794031, 1.2089037871812962, 1.2015371430751829, 1.1863242624380257, 1.1868085403518922, 1.445700732113183, 1.3240389411534295, 1.3537307236722433, 1.3255995623376757, 1.3495247430649371, 1.3789155158580684, 1.4488499615879706, 1.5647711045656405, 1.4623366189572704, 1.3600538453293456, 1.2908669454412627, 1.6933429133542253, 1.3262334631459152, 1.3295516076770675, 1.4587015676441375, 1.3709192032125819, 1.4192664901239007, 1.5108860724576942, 1.4833320062510338, 1.5689881701925188, 1.300823859976092, 1.4004695162065499, 1.4386779031433952, 1.3309960905020626, 1.3584264740972158, 1.4268956185252266, 1.4761362643256593, 1.2202804524237967, 1.2382277802688995, 1.2723474955832996, 1.2229670309476444, 1.1951522515732302, 1.1339243482700221, 1.0225403356102378, 1.1640194338154002, 1.00397309936361, 1.0094256315082939, 0.94983327984708721, 0.94566417251990875, 0.97882435604561124, 0.95459382965281891, 1.0461968249381057, 1.1696438024383595, 1.2055643736667681, 1.2165307533072145, 1.1701780497941419, 1.2439663291152796, 1.3621696720066061, 1.3612839899839642, 1.3539154697716822, 1.4133870654864635, 1.4971754404419215, 1.3529968027605037, 1.5026037800262386, 1.3329290349474743, 1.4464902003608393, 1.3724511901959304, 1.5011806735437296, 1.6573689841811645, 1.4877977987133952, 1.5178513638157756, 1.5489698620773131, 1.500371752021888, 1.8129857409902124, 1.5468590561030549, 1.4290517120185668, 1.6192892873034521, 1.5260010745664496, 1.5033402489991814, 1.6512632687692637, 1.4822015181808421, 1.537714577908315, 1.5216236862772188]\n",
      "[0.29360331339162449, 0.53175333640128852, 0.59341923607915326, 0.62908421537045556, 0.67211228716060745, 0.6730326737229636, 0.71329958582604691, 0.72089277496548554, 0.73999079613437646, 0.76000920386562354, 0.75655775425678784, 0.757478140819144, 0.7850897376898297, 0.77128393925448691, 0.77289461573861018, 0.78647031753336405, 0.81316152784169349, 0.81040036815462491, 0.80947998159226875, 0.82765761619880351, 0.82075471698113212, 0.81730326737229642, 0.81822365393465257, 0.82949838932351583, 0.8338702254947078, 0.83663138518177638, 0.84422457432121489, 0.82535664979291301, 0.84468476760239297, 0.84008283479061208, 0.83893235158766677, 0.839622641509434, 0.85112747353888629, 0.84031293143120112, 0.83870225494707773, 0.83294983893235164, 0.859180855959503, 0.86355269213069485, 0.81316152784169349, 0.86861481822365394, 0.86907501150483202, 0.8534284399447768, 0.86309249884951678, 0.85687988955361249, 0.84698573400828348, 0.85780027611596865, 0.84905660377358494, 0.88127013345605154, 0.86378278877128389, 0.86286240220892774, 0.87459733087896918, 0.86516336861481824, 0.85365853658536583, 0.86171191900598254, 0.88403129314312012, 0.88127013345605154, 0.87666820064427065, 0.87850897376898296, 0.87735849056603776, 0.88702254947077774, 0.90105844454670958, 0.87896916705016104, 0.8902439024390244, 0.88748274275195582, 0.90381960423377816, 0.90427979751495624, 0.90036815462494246, 0.90796134376438109, 0.90082834790612054, 0.88909341923607921, 0.88771283939254486, 0.8856419696272434, 0.89783709157846292, 0.88587206626783244, 0.87206626783248964, 0.89346525540727106, 0.88932351587666825, 0.87482742751955822, 0.86930510814542106, 0.89093419236079152, 0.87229636447307868, 0.88771283939254486, 0.87942936033133912, 0.88771283939254486, 0.87781868384721584, 0.86332259549010582, 0.88426138978370916, 0.8810400368154625, 0.88080994017487346, 0.88656235618959967, 0.8488265071329959, 0.87781868384721584, 0.89691670501610676, 0.87045559134836636, 0.87896916705016104, 0.88587206626783244, 0.86332259549010582, 0.89461573861021626, 0.87666820064427065, 0.88173032673722962]\n",
      "[3.067116605736131, 2.4113342216204368, 2.0623680981862429, 2.0190751005832412, 2.3435059182113789, 2.3125674764512687, 2.4037953910650938, 2.5343887487748513, 2.3657123212263422, 2.1596152124532311, 2.1853741821617882, 2.3112081027718281, 2.2310420151471804, 2.2618321974218598, 2.386134241548568, 2.2106865495313297, 2.5913654324847695, 2.4047255878177674, 2.3281171460476906, 1.9739104420260278, 2.1495953420345035, 2.3289112009940354, 2.6496912596645243, 2.5659068440098145, 2.2749584198018553, 2.591054626455604, 2.4754804177789622, 3.0402322811675764, 2.450150664208488, 2.4874324451953926, 2.6510030761078038, 2.8597608216038868, 2.7950061237646522, 3.0731054820384425, 2.8620266194438946, 2.7465700134831526, 2.8277330113148684, 2.8189921235035165, 2.6405635749131138, 2.6968223696369997, 2.9325561217058298, 2.6551111469848498, 2.83845118207859, 2.8499179259027985, 2.7724161945829753, 2.5886822770552, 3.0446770746925114, 2.4303561775706974, 3.0292621264254191, 2.6807993140640316, 2.7052297708536677, 2.6996558483341944, 3.0213645241310383, 2.8163235146801409, 2.4915705793639265, 2.3774556949414327, 2.6035222077128326, 2.5052926567424967, 2.3972111574337154, 2.2304132531735554, 2.7589478248052068, 2.3363630964703854, 2.7714739733972817, 2.799241908522704, 2.9061634100465468, 2.1585666691860892, 2.2025772086217019, 2.4803301425596698, 2.3720525034518749, 2.6326678767715301, 2.8682768754674837, 2.6327264145099885, 2.6934792102890608, 2.7886458992894543, 2.7573747096119061, 2.7216097512017341, 2.8412252620399041, 2.8711042835049123, 2.8731988049047237, 3.0319081726450436, 2.6692195423615286, 2.9898794963715796, 2.6804485044062432, 3.0135351297192758, 2.9348904665501512, 2.9440718080585242, 2.9114284705024045, 3.0579287603483061, 3.2395497250539678, 2.8902832659523612, 3.4346569459904428, 2.8688441877218089, 3.1550021775925399, 3.5723133337650355, 2.9671712891896687, 3.5408221881349107, 3.4903158758323265, 3.0724507857046053, 3.2438974310600206, 2.8904669702370192]\n",
      "[0.41031149305027104, 0.52201933411343138, 0.55424274976348264, 0.5918367347258886, 0.61009667027905723, 0.63050483354436337, 0.63157894740043219, 0.60687432871085101, 0.61654135341546978, 0.661654135370357, 0.66058002154629925, 0.6627282492264257, 0.67454350164318189, 0.66809881850676944, 0.65306122452180704, 0.67883995706745681, 0.64232008596111956, 0.65628356609001326, 0.68206229863566303, 0.70998925889345044, 0.69495166490848803, 0.68206229863566303, 0.67132116007497566, 0.65198711066573822, 0.70247046190096918, 0.69172932334028181, 0.66380236308249441, 0.62083780883974471, 0.70247046190096918, 0.67561761549925059, 0.66809881850676944, 0.66595059079463192, 0.6766917293553194, 0.65198711066573822, 0.67024704621890685, 0.68206229863566303, 0.67883995706745681, 0.67024704621890685, 0.68743286791600677, 0.68528464023588043, 0.67454350164318189, 0.69924812033276296, 0.67883995706745681, 0.67991407095553669, 0.70891514503738162, 0.70247046190096918, 0.6766917293553194, 0.71535982817379407, 0.67776584321138811, 0.69172932334028181, 0.7056928034691754, 0.69817400647669425, 0.67776584321138811, 0.70676691732524421, 0.7185821697420004, 0.74006444686337525, 0.7056928034691754, 0.7185821697420004, 0.72502685287841284, 0.74543501614371888, 0.68528464020386926, 0.74006444686337525, 0.68421052637981161, 0.69924812033276296, 0.68313641245972068, 0.74113856071944395, 0.73791621915123773, 0.71643394202986288, 0.7336197637269628, 0.71750805588593158, 0.690655209484213, 0.7185821697420004, 0.71106337274951914, 0.70891514503738162, 0.71428571431772536, 0.71750805588593158, 0.72073039745413781, 0.72502685287841284, 0.71535982817379407, 0.70354457575703799, 0.72502685287841284, 0.69387755105241922, 0.73469387758303151, 0.70032223418883166, 0.72932330830268777, 0.72287862516627532, 0.71643394202986288, 0.71428571431772536, 0.70139634804490048, 0.72073039745413781, 0.6627282492264257, 0.71535982817379407, 0.70247046190096918, 0.67239527393104437, 0.71535982817379407, 0.67561761549925059, 0.66702470465070063, 0.7046186896131067, 0.69172932334028181, 0.72824919444661906]\n",
      "('Test loss:', 2.799822656893423)\n",
      "('Test accuracy:', 0.72317596566523601)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "new_classification_layer = Dense(num_classes, activation='softmax')\n",
    "inp = vgg.input\n",
    "out = new_classification_layer(vgg.layers[-2].output)\n",
    "model_new = Model(inp, out)\n",
    "\n",
    "\n",
    "for l, layer in enumerate(model_new.layers[:-1]):\n",
    "    layer.trainable = False\n",
    "for l, layer in enumerate(model_new.layers[-1:]):\n",
    "    layer.trainable = True\n",
    "    \n",
    "model_new.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_new.summary()\n",
    "\n",
    "history = model_new.fit(x_train, y_train, \n",
    "                        batch_size=8, \n",
    "                        epochs=100, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "# with adam - 16\n",
    "print(history.history['loss'])\n",
    "print(history.history['acc'])\n",
    "\n",
    "print(history.history['val_loss'])\n",
    "print(history.history['val_acc'])\n",
    "\n",
    "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 97)                397409    \n",
      "=================================================================\n",
      "Total params: 134,657,953.0\n",
      "Trainable params: 397,409.0\n",
      "Non-trainable params: 134,260,544.0\n",
      "_________________________________________________________________\n",
      "Train on 4346 samples, validate on 931 samples\n",
      "Epoch 1/100\n",
      "4346/4346 [==============================] - 47s - loss: 3.3656 - acc: 0.2579 - val_loss: 2.6214 - val_acc: 0.3706\n",
      "Epoch 2/100\n",
      "4346/4346 [==============================] - 47s - loss: 2.1247 - acc: 0.5092 - val_loss: 2.0541 - val_acc: 0.4984\n",
      "Epoch 3/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.6840 - acc: 0.6017 - val_loss: 1.7448 - val_acc: 0.5532\n",
      "Epoch 4/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.4197 - acc: 0.6675 - val_loss: 1.5364 - val_acc: 0.6187\n",
      "Epoch 5/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.2458 - acc: 0.7057 - val_loss: 1.4348 - val_acc: 0.6262\n",
      "Epoch 6/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.1295 - acc: 0.7260 - val_loss: 1.3367 - val_acc: 0.6702\n",
      "Epoch 7/100\n",
      "4346/4346 [==============================] - 47s - loss: 1.0282 - acc: 0.7531 - val_loss: 1.2644 - val_acc: 0.6810\n",
      "Epoch 8/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.9609 - acc: 0.7618 - val_loss: 1.2164 - val_acc: 0.6799\n",
      "Epoch 9/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.8986 - acc: 0.7775 - val_loss: 1.1530 - val_acc: 0.6939\n",
      "Epoch 10/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.8526 - acc: 0.7837 - val_loss: 1.1422 - val_acc: 0.6950\n",
      "Epoch 11/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.8020 - acc: 0.7938 - val_loss: 1.0899 - val_acc: 0.7132\n",
      "Epoch 12/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.7685 - acc: 0.8044 - val_loss: 1.0822 - val_acc: 0.7068\n",
      "Epoch 13/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.7335 - acc: 0.8099 - val_loss: 1.0856 - val_acc: 0.7014\n",
      "Epoch 14/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.7069 - acc: 0.8152 - val_loss: 0.9948 - val_acc: 0.7476\n",
      "Epoch 15/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.6770 - acc: 0.8249 - val_loss: 1.0178 - val_acc: 0.7272\n",
      "Epoch 16/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.6515 - acc: 0.8219 - val_loss: 1.0231 - val_acc: 0.7218\n",
      "Epoch 17/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.6236 - acc: 0.8311 - val_loss: 1.0157 - val_acc: 0.7207\n",
      "Epoch 18/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.6090 - acc: 0.8403 - val_loss: 0.9221 - val_acc: 0.7497\n",
      "Epoch 19/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.5836 - acc: 0.8456 - val_loss: 0.9853 - val_acc: 0.7379\n",
      "Epoch 20/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.5689 - acc: 0.8486 - val_loss: 0.9893 - val_acc: 0.7315\n",
      "Epoch 21/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.5580 - acc: 0.8534 - val_loss: 0.9742 - val_acc: 0.7272\n",
      "Epoch 22/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.5381 - acc: 0.8532 - val_loss: 0.9157 - val_acc: 0.7594\n",
      "Epoch 23/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.5220 - acc: 0.8603 - val_loss: 0.9385 - val_acc: 0.7551\n",
      "Epoch 24/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.5076 - acc: 0.8624 - val_loss: 0.9025 - val_acc: 0.7669\n",
      "Epoch 25/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.4978 - acc: 0.8698 - val_loss: 0.9062 - val_acc: 0.7562\n",
      "Epoch 26/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.4881 - acc: 0.8684 - val_loss: 0.8853 - val_acc: 0.7573\n",
      "Epoch 27/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.4745 - acc: 0.8682 - val_loss: 0.8906 - val_acc: 0.7487\n",
      "Epoch 28/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.4601 - acc: 0.8776 - val_loss: 0.9194 - val_acc: 0.7540\n",
      "Epoch 29/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.4538 - acc: 0.8808 - val_loss: 0.8939 - val_acc: 0.7519\n",
      "Epoch 30/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.4388 - acc: 0.8829 - val_loss: 0.9082 - val_acc: 0.7573\n",
      "Epoch 31/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.4338 - acc: 0.8854 - val_loss: 0.8911 - val_acc: 0.7583\n",
      "Epoch 32/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.4226 - acc: 0.8884 - val_loss: 0.8815 - val_acc: 0.7487\n",
      "Epoch 33/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.4172 - acc: 0.8884 - val_loss: 0.8935 - val_acc: 0.7562\n",
      "Epoch 34/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.4066 - acc: 0.8907 - val_loss: 0.8558 - val_acc: 0.7734\n",
      "Epoch 35/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3988 - acc: 0.8937 - val_loss: 0.8551 - val_acc: 0.7573\n",
      "Epoch 36/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3927 - acc: 0.8983 - val_loss: 0.8572 - val_acc: 0.7766\n",
      "Epoch 37/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3857 - acc: 0.8958 - val_loss: 0.8526 - val_acc: 0.7615\n",
      "Epoch 38/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3741 - acc: 0.8976 - val_loss: 0.8582 - val_acc: 0.7734\n",
      "Epoch 39/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3701 - acc: 0.8985 - val_loss: 0.8561 - val_acc: 0.7744\n",
      "Epoch 40/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3596 - acc: 0.9006 - val_loss: 0.8460 - val_acc: 0.7701\n",
      "Epoch 41/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3523 - acc: 0.9029 - val_loss: 0.8881 - val_acc: 0.7712\n",
      "Epoch 42/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3474 - acc: 0.9040 - val_loss: 0.8304 - val_acc: 0.7798\n",
      "Epoch 43/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3431 - acc: 0.9017 - val_loss: 0.8692 - val_acc: 0.7573\n",
      "Epoch 44/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3379 - acc: 0.9084 - val_loss: 0.8625 - val_acc: 0.7669\n",
      "Epoch 45/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3292 - acc: 0.9156 - val_loss: 0.8258 - val_acc: 0.7916\n",
      "Epoch 46/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3234 - acc: 0.9116 - val_loss: 0.8565 - val_acc: 0.7755\n",
      "Epoch 47/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3180 - acc: 0.9107 - val_loss: 0.8448 - val_acc: 0.7744\n",
      "Epoch 48/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3127 - acc: 0.9167 - val_loss: 0.8226 - val_acc: 0.7744\n",
      "Epoch 49/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3112 - acc: 0.9151 - val_loss: 0.8462 - val_acc: 0.7669\n",
      "Epoch 50/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3068 - acc: 0.9149 - val_loss: 0.8371 - val_acc: 0.7712\n",
      "Epoch 51/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.3050 - acc: 0.9144 - val_loss: 0.8216 - val_acc: 0.7777\n",
      "Epoch 52/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2962 - acc: 0.9181 - val_loss: 0.8531 - val_acc: 0.7744\n",
      "Epoch 53/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2908 - acc: 0.9264 - val_loss: 0.8201 - val_acc: 0.7820\n",
      "Epoch 54/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2874 - acc: 0.9229 - val_loss: 0.8696 - val_acc: 0.7787\n",
      "Epoch 55/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2841 - acc: 0.9243 - val_loss: 0.8048 - val_acc: 0.7863\n",
      "Epoch 56/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2795 - acc: 0.9234 - val_loss: 0.8184 - val_acc: 0.7777\n",
      "Epoch 57/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2757 - acc: 0.9287 - val_loss: 0.8616 - val_acc: 0.7830\n",
      "Epoch 58/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2693 - acc: 0.9282 - val_loss: 0.8615 - val_acc: 0.7680\n",
      "Epoch 59/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2657 - acc: 0.9273 - val_loss: 0.8328 - val_acc: 0.7852\n",
      "Epoch 60/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2657 - acc: 0.9277 - val_loss: 0.8373 - val_acc: 0.7873\n",
      "Epoch 61/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2599 - acc: 0.9289 - val_loss: 0.8574 - val_acc: 0.7777\n",
      "Epoch 62/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2576 - acc: 0.9303 - val_loss: 0.8263 - val_acc: 0.7863\n",
      "Epoch 63/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2540 - acc: 0.9289 - val_loss: 0.8170 - val_acc: 0.7809\n",
      "Epoch 64/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2499 - acc: 0.9317 - val_loss: 0.8319 - val_acc: 0.7809\n",
      "Epoch 65/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2441 - acc: 0.9330 - val_loss: 0.8596 - val_acc: 0.7830\n",
      "Epoch 66/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2465 - acc: 0.9298 - val_loss: 0.8314 - val_acc: 0.7637\n",
      "Epoch 67/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2406 - acc: 0.9330 - val_loss: 0.8166 - val_acc: 0.7820\n",
      "Epoch 68/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2362 - acc: 0.9360 - val_loss: 0.8219 - val_acc: 0.7798\n",
      "Epoch 69/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2359 - acc: 0.9386 - val_loss: 0.8563 - val_acc: 0.7744\n",
      "Epoch 70/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2341 - acc: 0.9386 - val_loss: 0.8141 - val_acc: 0.7884\n",
      "Epoch 71/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2278 - acc: 0.9381 - val_loss: 0.8623 - val_acc: 0.7777\n",
      "Epoch 72/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2277 - acc: 0.9393 - val_loss: 0.8125 - val_acc: 0.7863\n",
      "Epoch 73/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2216 - acc: 0.9427 - val_loss: 0.8232 - val_acc: 0.7830\n",
      "Epoch 74/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2191 - acc: 0.9388 - val_loss: 0.8015 - val_acc: 0.7841\n",
      "Epoch 75/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2170 - acc: 0.9411 - val_loss: 0.7985 - val_acc: 0.7884\n",
      "Epoch 76/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2137 - acc: 0.9397 - val_loss: 0.8102 - val_acc: 0.7927\n",
      "Epoch 77/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2118 - acc: 0.9427 - val_loss: 0.8484 - val_acc: 0.7734\n",
      "Epoch 78/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2105 - acc: 0.9434 - val_loss: 0.8083 - val_acc: 0.7991\n",
      "Epoch 79/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2049 - acc: 0.9450 - val_loss: 0.8279 - val_acc: 0.7734\n",
      "Epoch 80/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2043 - acc: 0.9422 - val_loss: 0.8212 - val_acc: 0.7873\n",
      "Epoch 81/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.2017 - acc: 0.9455 - val_loss: 0.8571 - val_acc: 0.7723\n",
      "Epoch 82/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1995 - acc: 0.9473 - val_loss: 0.8530 - val_acc: 0.7766\n",
      "Epoch 83/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1948 - acc: 0.9457 - val_loss: 0.8886 - val_acc: 0.7755\n",
      "Epoch 84/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1965 - acc: 0.9473 - val_loss: 0.8364 - val_acc: 0.7723\n",
      "Epoch 85/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1936 - acc: 0.9455 - val_loss: 0.8503 - val_acc: 0.7820\n",
      "Epoch 86/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1919 - acc: 0.9496 - val_loss: 0.8187 - val_acc: 0.7905\n",
      "Epoch 87/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1923 - acc: 0.9524 - val_loss: 0.8278 - val_acc: 0.7830\n",
      "Epoch 88/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1856 - acc: 0.9505 - val_loss: 0.8340 - val_acc: 0.7948\n",
      "Epoch 89/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1851 - acc: 0.9510 - val_loss: 0.8342 - val_acc: 0.7787\n",
      "Epoch 90/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1815 - acc: 0.9524 - val_loss: 0.8281 - val_acc: 0.7787\n",
      "Epoch 91/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1839 - acc: 0.9512 - val_loss: 0.8119 - val_acc: 0.7884\n",
      "Epoch 92/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1770 - acc: 0.9563 - val_loss: 0.8291 - val_acc: 0.7916\n",
      "Epoch 93/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1770 - acc: 0.9549 - val_loss: 0.8288 - val_acc: 0.7798\n",
      "Epoch 94/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1765 - acc: 0.9524 - val_loss: 0.7956 - val_acc: 0.7884\n",
      "Epoch 95/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1756 - acc: 0.9544 - val_loss: 0.8324 - val_acc: 0.7820\n",
      "Epoch 96/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1720 - acc: 0.9528 - val_loss: 0.8336 - val_acc: 0.7895\n",
      "Epoch 97/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1713 - acc: 0.9572 - val_loss: 0.8244 - val_acc: 0.7884\n",
      "Epoch 98/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1690 - acc: 0.9561 - val_loss: 0.8092 - val_acc: 0.7905\n",
      "Epoch 99/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1665 - acc: 0.9544 - val_loss: 0.8159 - val_acc: 0.7916\n",
      "Epoch 100/100\n",
      "4346/4346 [==============================] - 47s - loss: 0.1646 - acc: 0.9590 - val_loss: 0.8500 - val_acc: 0.7873\n",
      "[3.3655898688413335, 2.1246672944983085, 1.6839932685506218, 1.4197106364406422, 1.2457895418547618, 1.1294656784727559, 1.0281734532555347, 0.96089229241418117, 0.89862916641557922, 0.8525862156825369, 0.80195188295015585, 0.76853495744711453, 0.73354200315925311, 0.70687844245268128, 0.67700013239236678, 0.65145068371893899, 0.6236441373838788, 0.60900553658062884, 0.58363955565657533, 0.56888127757996676, 0.55803823082853976, 0.53814766623227206, 0.52198453520815113, 0.50758873493000489, 0.49776734502920073, 0.48809678917703569, 0.47454042085204262, 0.46010626984517583, 0.45384024763526976, 0.43875127569315858, 0.4338310632573546, 0.42258426289211698, 0.41716331311403415, 0.40661625689417602, 0.39883654838744198, 0.39265681418932691, 0.38567612225209519, 0.3741476734584987, 0.37007717218368325, 0.3595799151195142, 0.35228337102012047, 0.34744700742478313, 0.3430990455007833, 0.33794524568294632, 0.32919573208494596, 0.32341186046339759, 0.31797298449373412, 0.31267481111750794, 0.31124715965105759, 0.30675727816502613, 0.30502131869960741, 0.29621559206481629, 0.29081546827581584, 0.287381146933748, 0.28408327232746283, 0.27951331307500382, 0.27574795668675134, 0.2692546964650035, 0.26570822636340136, 0.26574014872312546, 0.25992342032294152, 0.2576241568270925, 0.25399663063212785, 0.24989267589641823, 0.24410106883299856, 0.24654540426093097, 0.24056961502450414, 0.23624903440400033, 0.23589195488499726, 0.23414666121448419, 0.22775431478192906, 0.22765205753560008, 0.22160117874724308, 0.21905026584692228, 0.2170008645186482, 0.21367020292050734, 0.21180104455549323, 0.21054368153140737, 0.20486183613804468, 0.20427147414855867, 0.20173387966871784, 0.19947347733814885, 0.19481594149354164, 0.19651352704141128, 0.19363732735450251, 0.19190765572180377, 0.19232152191642629, 0.18561064373801242, 0.18508813976077898, 0.18151701310434362, 0.18394253033248034, 0.17703882184512745, 0.17697631244732387, 0.17648551523303155, 0.17560988544525893, 0.17195521085201629, 0.17129910417827721, 0.16895690073893058, 0.16646286234050883, 0.16462715600726091]\n",
      "[0.25793833410032213, 0.5092038656235619, 0.601702715140359, 0.66751035434882655, 0.70570639668660839, 0.72595490105844451, 0.75310630464795214, 0.76184997699033596, 0.77749654855039119, 0.78370915784629547, 0.79383341003221353, 0.80441785549930966, 0.80994017487344683, 0.81523239760699495, 0.82489645651173493, 0.82190520018407731, 0.83110906580763921, 0.84031293143120112, 0.84560515416474924, 0.84859641049240686, 0.8534284399447768, 0.85319834330418776, 0.86033133916244819, 0.86240220892774966, 0.86976530142659914, 0.8683847215830649, 0.86815462494247586, 0.8775885872066268, 0.88080994017487346, 0.88288080994017493, 0.88541187298665436, 0.88840312931431198, 0.88840312931431198, 0.89070409572020248, 0.8936953520478601, 0.898297284859641, 0.89576622181316157, 0.89760699493787388, 0.89852738150023015, 0.9005982512655315, 0.90289921767142201, 0.9040497008743672, 0.90174873446847681, 0.90842153704555917, 0.91555453290381961, 0.91164289001380583, 0.91072250345144956, 0.9167050161067648, 0.91509433962264153, 0.91486424298205249, 0.91440404970087441, 0.91808559595029915, 0.92636907501150478, 0.92291762540266908, 0.92429820524620343, 0.92337781868384716, 0.92867004141739529, 0.92820984813621721, 0.92728946157386105, 0.92774965485503913, 0.92890013805798433, 0.93028071790151867, 0.92890013805798433, 0.93166129774505291, 0.93304187758858725, 0.92982052462034059, 0.93304187758858725, 0.93603313391624487, 0.93856419696272431, 0.93856419696272431, 0.93810400368154623, 0.93925448688449154, 0.94270593649332723, 0.93879429360331335, 0.94109526000920385, 0.93971468016566961, 0.94270593649332723, 0.94339622641509435, 0.94500690289921763, 0.94224574321214916, 0.94546709618039582, 0.94730786930510813, 0.94569719282098486, 0.94730786930510813, 0.94546709618039582, 0.94960883571099863, 0.95236999539806722, 0.95052922227335479, 0.95098941555453287, 0.95236999539806722, 0.95121951219512191, 0.95628163828808099, 0.95490105844454676, 0.95236999539806722, 0.95444086516336857, 0.95283018867924529, 0.95720202485043715, 0.95605154164749195, 0.95444086516336857, 0.95904279797514957]\n",
      "[2.6213751215683785, 2.0540768641790437, 1.7448405714987685, 1.5364245230093934, 1.4348457840408342, 1.3367288663231085, 1.2643790967226285, 1.2164010086710035, 1.1530268334420488, 1.1421646485395258, 1.0898848193288491, 1.0821822802951333, 1.0855635106243458, 0.99481318218815595, 1.0177913412179649, 1.0231112171960826, 1.0156741016789486, 0.92206450472520829, 0.98530842418253228, 0.98933906275398875, 0.97419061056397271, 0.915709945364234, 0.93850709040170066, 0.90254124006702618, 0.90620057232013707, 0.88528938208147478, 0.89064791207987049, 0.9193740793364773, 0.89389379784288781, 0.9082044734830631, 0.89113051334654092, 0.8814535100977352, 0.89350298648874182, 0.85581759858784434, 0.85507816340370157, 0.85716946839197883, 0.85256120444112382, 0.8582395059081589, 0.85605557507782315, 0.84603118805252775, 0.88810953858011388, 0.83036820119532939, 0.86915087506223054, 0.86245698874734267, 0.82584185155646761, 0.85648609971513556, 0.84480409330571371, 0.82264086751369347, 0.84622549813412828, 0.83705238368918378, 0.82157766871818538, 0.85312999295176806, 0.82013660032306146, 0.86961446107774332, 0.8048324830085839, 0.81838066628248807, 0.86159466199024659, 0.86151240109421001, 0.8327817618686838, 0.83728788829899758, 0.85740857126009196, 0.82632909132173704, 0.81698585203203555, 0.83188527313518212, 0.85961085619959099, 0.8314065429773162, 0.81655652960312231, 0.82189250746231712, 0.85631030158536903, 0.81412274494895231, 0.86230833746820978, 0.81254887764935335, 0.82316503368186034, 0.8015253006967803, 0.79854808732963278, 0.81018564238680169, 0.84842961660869398, 0.80825374597157362, 0.82791067787499095, 0.82116943348107097, 0.85705744728013855, 0.85295650361567965, 0.88856719662340322, 0.83638449879133103, 0.85026662933583164, 0.81867114716998746, 0.82780276452592316, 0.8340132576958641, 0.83419079516006978, 0.82814090304420795, 0.8118564855991578, 0.82913186693229946, 0.82884640144394306, 0.79556799767424868, 0.83235968366889512, 0.833618720515234, 0.82441658544265106, 0.80924630381975216, 0.8159329851671141, 0.84999795478721629]\n",
      "[0.37056928037572751, 0.49838882924790795, 0.55316863590741383, 0.61868958112760719, 0.62620837812008845, 0.67024704621890685, 0.68098818481160539, 0.67991407092352563, 0.69387755105241922, 0.69495166490848803, 0.71321160049366772, 0.70676691732524421, 0.70139634807691154, 0.74758324388786745, 0.72717508059055025, 0.72180451131020662, 0.72073039745413781, 0.74973147156799391, 0.73791621915123773, 0.73147153601482529, 0.72717508062256142, 0.75939849627261258, 0.75510204084833765, 0.7669172932971049, 0.75617615473641742, 0.75725026856047506, 0.74865735774393627, 0.75402792699226884, 0.75187969931214249, 0.75725026859248623, 0.75832438244855493, 0.74865735771192521, 0.75617615470440636, 0.77336197643351734, 0.75725026859248623, 0.77658431800172356, 0.76154672401676116, 0.77336197640150628, 0.77443609028958604, 0.77013963483330006, 0.77121374868936876, 0.77980665956992978, 0.75725026856047506, 0.7669172932971049, 0.79162191198668597, 0.77551020414565486, 0.77443609028958604, 0.77443609028958604, 0.7669172932971049, 0.77121374872137982, 0.77765843185779226, 0.77443609028958604, 0.7819548872820673, 0.77873254568185002, 0.78625134270634223, 0.7776584318257812, 0.78302900110612494, 0.76799140712116254, 0.78517722885027352, 0.78732545653039987, 0.77765843185779226, 0.78625134270634223, 0.78088077339398743, 0.7808807734259986, 0.783029001138136, 0.76369495169688761, 0.7819548872820673, 0.77980665953791872, 0.77443609028958604, 0.78839957041847974, 0.77765843185779226, 0.78625134270634223, 0.783029001138136, 0.78410311499420482, 0.78839957041847974, 0.79269602584275467, 0.77336197643351734, 0.79914070897916722, 0.77336197643351734, 0.78732545656241104, 0.77228786257744864, 0.77658431793770144, 0.77551020408163263, 0.77228786254543746, 0.7819548872820673, 0.79054779813061726, 0.78302900110612494, 0.79484425355489219, 0.77873254568185002, 0.77873254571386108, 0.78839957041847974, 0.79162191198668597, 0.77980665956992978, 0.78839957041847974, 0.7819548872820673, 0.78947368427454845, 0.78839957035445762, 0.79054779813061726, 0.79162191198668597, 0.78732545656241104]\n",
      "('Test loss:', 0.83147614324553332)\n",
      "('Test accuracy:', 0.79077253218884125)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_classification_layer = Dense(num_classes, activation='softmax')\n",
    "inp = vgg.input\n",
    "out = new_classification_layer(vgg.layers[-2].output)\n",
    "model_new = Model(inp, out)\n",
    "\n",
    "\n",
    "for l, layer in enumerate(model_new.layers[:-1]):\n",
    "    layer.trainable = False\n",
    "for l, layer in enumerate(model_new.layers[-1:]):\n",
    "    layer.trainable = True\n",
    "    \n",
    "model_new.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_new.summary()\n",
    "\n",
    "history = model_new.fit(x_train, y_train, \n",
    "                        batch_size=8, \n",
    "                        epochs=100, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "# with adam - 16\n",
    "print(history.history['loss'])\n",
    "print(history.history['acc'])\n",
    "\n",
    "print(history.history['val_loss'])\n",
    "print(history.history['val_acc'])\n",
    "\n",
    "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 97)                397409    \n",
      "=================================================================\n",
      "Total params: 134,657,953.0\n",
      "Trainable params: 397,409.0\n",
      "Non-trainable params: 134,260,544.0\n",
      "_________________________________________________________________\n",
      "Train on 4346 samples, validate on 931 samples\n",
      "Epoch 1/100\n",
      "4346/4346 [==============================] - 37s - loss: 3.8167 - acc: 0.2283 - val_loss: 2.8009 - val_acc: 0.3845\n",
      "Epoch 2/100\n",
      "4346/4346 [==============================] - 37s - loss: 2.1635 - acc: 0.5074 - val_loss: 1.9999 - val_acc: 0.5532\n",
      "Epoch 3/100\n",
      "4346/4346 [==============================] - 37s - loss: 1.6736 - acc: 0.6091 - val_loss: 1.7225 - val_acc: 0.5779\n",
      "Epoch 4/100\n",
      "4346/4346 [==============================] - 37s - loss: 1.3836 - acc: 0.6675 - val_loss: 1.4323 - val_acc: 0.6617\n",
      "Epoch 5/100\n",
      "4346/4346 [==============================] - 37s - loss: 1.2018 - acc: 0.7059 - val_loss: 1.3281 - val_acc: 0.6670\n",
      "Epoch 6/100\n",
      "4346/4346 [==============================] - 37s - loss: 1.0565 - acc: 0.7382 - val_loss: 1.2632 - val_acc: 0.6960\n",
      "Epoch 7/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.9394 - acc: 0.7711 - val_loss: 1.2774 - val_acc: 0.6724\n",
      "Epoch 8/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.8486 - acc: 0.7936 - val_loss: 1.1596 - val_acc: 0.6950\n",
      "Epoch 9/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.8107 - acc: 0.7895 - val_loss: 1.1198 - val_acc: 0.7261\n",
      "Epoch 10/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.7456 - acc: 0.8083 - val_loss: 1.0472 - val_acc: 0.7293\n",
      "Epoch 11/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.6951 - acc: 0.8233 - val_loss: 1.0937 - val_acc: 0.7218\n",
      "Epoch 12/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.6505 - acc: 0.8362 - val_loss: 1.0862 - val_acc: 0.7089\n",
      "Epoch 13/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.6089 - acc: 0.8433 - val_loss: 0.9796 - val_acc: 0.7411\n",
      "Epoch 14/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.5465 - acc: 0.8659 - val_loss: 1.0251 - val_acc: 0.7336\n",
      "Epoch 15/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.5524 - acc: 0.8530 - val_loss: 1.0922 - val_acc: 0.7025\n",
      "Epoch 16/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.5464 - acc: 0.8539 - val_loss: 1.0141 - val_acc: 0.7347\n",
      "Epoch 17/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.4816 - acc: 0.8787 - val_loss: 0.9571 - val_acc: 0.7497\n",
      "Epoch 18/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.4575 - acc: 0.8836 - val_loss: 0.9352 - val_acc: 0.7573\n",
      "Epoch 19/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.4365 - acc: 0.8891 - val_loss: 1.0483 - val_acc: 0.7229\n",
      "Epoch 20/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.4353 - acc: 0.8902 - val_loss: 0.9701 - val_acc: 0.7508\n",
      "Epoch 21/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.4036 - acc: 0.8985 - val_loss: 0.9466 - val_acc: 0.7476\n",
      "Epoch 22/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.4103 - acc: 0.8953 - val_loss: 0.9039 - val_acc: 0.7551\n",
      "Epoch 23/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3575 - acc: 0.9110 - val_loss: 0.9858 - val_acc: 0.7422\n",
      "Epoch 24/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3426 - acc: 0.9160 - val_loss: 0.8730 - val_acc: 0.7637\n",
      "Epoch 25/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3241 - acc: 0.9252 - val_loss: 1.0002 - val_acc: 0.7454\n",
      "Epoch 26/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3219 - acc: 0.9195 - val_loss: 0.9251 - val_acc: 0.7583\n",
      "Epoch 27/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3009 - acc: 0.9296 - val_loss: 0.8977 - val_acc: 0.7562\n",
      "Epoch 28/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2849 - acc: 0.9347 - val_loss: 0.8723 - val_acc: 0.7594\n",
      "Epoch 29/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2845 - acc: 0.9284 - val_loss: 0.9267 - val_acc: 0.7487\n",
      "Epoch 30/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2743 - acc: 0.9314 - val_loss: 0.8382 - val_acc: 0.7809\n",
      "Epoch 31/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2603 - acc: 0.9397 - val_loss: 0.8726 - val_acc: 0.7648\n",
      "Epoch 32/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2410 - acc: 0.9457 - val_loss: 0.8680 - val_acc: 0.7830\n",
      "Epoch 33/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2614 - acc: 0.9351 - val_loss: 0.9074 - val_acc: 0.7594\n",
      "Epoch 34/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2355 - acc: 0.9459 - val_loss: 0.8811 - val_acc: 0.7734\n",
      "Epoch 35/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2212 - acc: 0.9510 - val_loss: 0.8957 - val_acc: 0.7540\n",
      "Epoch 36/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2304 - acc: 0.9471 - val_loss: 0.8380 - val_acc: 0.7744\n",
      "Epoch 37/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2181 - acc: 0.9491 - val_loss: 0.9055 - val_acc: 0.7648\n",
      "Epoch 38/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2055 - acc: 0.9501 - val_loss: 0.8366 - val_acc: 0.7830\n",
      "Epoch 39/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1996 - acc: 0.9577 - val_loss: 0.8022 - val_acc: 0.7852\n",
      "Epoch 40/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1998 - acc: 0.9542 - val_loss: 0.8499 - val_acc: 0.7830\n",
      "Epoch 41/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1784 - acc: 0.9609 - val_loss: 0.8713 - val_acc: 0.7798\n",
      "Epoch 42/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1610 - acc: 0.9673 - val_loss: 0.8424 - val_acc: 0.7852\n",
      "Epoch 43/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1689 - acc: 0.9653 - val_loss: 0.9217 - val_acc: 0.7573\n",
      "Epoch 44/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1700 - acc: 0.9623 - val_loss: 0.8677 - val_acc: 0.7680\n",
      "Epoch 45/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1539 - acc: 0.9701 - val_loss: 0.8846 - val_acc: 0.7809\n",
      "Epoch 46/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1475 - acc: 0.9719 - val_loss: 0.8038 - val_acc: 0.7938\n",
      "Epoch 47/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1353 - acc: 0.9770 - val_loss: 0.8507 - val_acc: 0.7734\n",
      "Epoch 48/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1331 - acc: 0.9772 - val_loss: 0.8731 - val_acc: 0.7820\n",
      "Epoch 49/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1376 - acc: 0.9724 - val_loss: 0.8307 - val_acc: 0.7766\n",
      "Epoch 50/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1298 - acc: 0.9758 - val_loss: 0.8774 - val_acc: 0.7734\n",
      "Epoch 51/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1460 - acc: 0.9687 - val_loss: 0.9164 - val_acc: 0.7680\n",
      "Epoch 52/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1366 - acc: 0.9712 - val_loss: 0.8597 - val_acc: 0.7766\n",
      "Epoch 53/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1302 - acc: 0.9733 - val_loss: 0.8223 - val_acc: 0.7841\n",
      "Epoch 54/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1150 - acc: 0.9832 - val_loss: 0.8211 - val_acc: 0.7916\n",
      "Epoch 55/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1496 - acc: 0.9664 - val_loss: 0.8380 - val_acc: 0.7873\n",
      "Epoch 56/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1153 - acc: 0.9793 - val_loss: 0.8825 - val_acc: 0.7777\n",
      "Epoch 57/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1140 - acc: 0.9751 - val_loss: 0.8530 - val_acc: 0.7755\n",
      "Epoch 58/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1019 - acc: 0.9830 - val_loss: 0.7915 - val_acc: 0.7991\n",
      "Epoch 59/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1062 - acc: 0.9791 - val_loss: 0.8904 - val_acc: 0.7766\n",
      "Epoch 60/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1021 - acc: 0.9788 - val_loss: 0.8847 - val_acc: 0.7873\n",
      "Epoch 61/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1098 - acc: 0.9791 - val_loss: 0.8464 - val_acc: 0.7873\n",
      "Epoch 62/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0919 - acc: 0.9844 - val_loss: 0.7984 - val_acc: 0.7916\n",
      "Epoch 63/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0903 - acc: 0.9830 - val_loss: 0.8603 - val_acc: 0.7884\n",
      "Epoch 64/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1011 - acc: 0.9807 - val_loss: 0.9026 - val_acc: 0.7701\n",
      "Epoch 65/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0836 - acc: 0.9862 - val_loss: 0.8959 - val_acc: 0.7873\n",
      "Epoch 66/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0974 - acc: 0.9811 - val_loss: 0.8828 - val_acc: 0.7820\n",
      "Epoch 67/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0934 - acc: 0.9823 - val_loss: 0.8740 - val_acc: 0.7863\n",
      "Epoch 68/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0743 - acc: 0.9894 - val_loss: 0.8707 - val_acc: 0.7777\n",
      "Epoch 69/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0850 - acc: 0.9867 - val_loss: 0.8871 - val_acc: 0.7863\n",
      "Epoch 70/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0804 - acc: 0.9853 - val_loss: 0.8318 - val_acc: 0.8077\n",
      "Epoch 71/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0662 - acc: 0.9903 - val_loss: 0.8091 - val_acc: 0.7895\n",
      "Epoch 72/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0607 - acc: 0.9915 - val_loss: 0.8192 - val_acc: 0.7948\n",
      "Epoch 73/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0846 - acc: 0.9830 - val_loss: 0.9494 - val_acc: 0.7648\n",
      "Epoch 74/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0962 - acc: 0.9802 - val_loss: 0.9066 - val_acc: 0.7830\n",
      "Epoch 75/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0646 - acc: 0.9908 - val_loss: 0.8530 - val_acc: 0.7820\n",
      "Epoch 76/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0673 - acc: 0.9906 - val_loss: 0.9142 - val_acc: 0.7777\n",
      "Epoch 77/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1100 - acc: 0.9793 - val_loss: 0.8963 - val_acc: 0.7884\n",
      "Epoch 78/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0834 - acc: 0.9818 - val_loss: 0.9045 - val_acc: 0.7777\n",
      "Epoch 79/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0915 - acc: 0.9823 - val_loss: 0.9649 - val_acc: 0.7830\n",
      "Epoch 80/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.1351 - acc: 0.9685 - val_loss: 1.0007 - val_acc: 0.7798\n",
      "Epoch 81/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0898 - acc: 0.9807 - val_loss: 0.9000 - val_acc: 0.7981\n",
      "Epoch 82/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0805 - acc: 0.9834 - val_loss: 0.9802 - val_acc: 0.7723\n",
      "Epoch 83/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0538 - acc: 0.9919 - val_loss: 0.8524 - val_acc: 0.8034\n",
      "Epoch 84/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0442 - acc: 0.9949 - val_loss: 0.8567 - val_acc: 0.7948\n",
      "Epoch 85/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0395 - acc: 0.9968 - val_loss: 0.8937 - val_acc: 0.7873\n",
      "Epoch 86/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0405 - acc: 0.9961 - val_loss: 0.9115 - val_acc: 0.7798\n",
      "Epoch 87/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0539 - acc: 0.9901 - val_loss: 0.8876 - val_acc: 0.7927\n",
      "Epoch 88/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0542 - acc: 0.9887 - val_loss: 0.8698 - val_acc: 0.7916\n",
      "Epoch 89/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0420 - acc: 0.9956 - val_loss: 0.8866 - val_acc: 0.7830\n",
      "Epoch 90/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0429 - acc: 0.9931 - val_loss: 0.8649 - val_acc: 0.7852\n",
      "Epoch 91/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0463 - acc: 0.9924 - val_loss: 0.9181 - val_acc: 0.7905\n",
      "Epoch 92/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0474 - acc: 0.9917 - val_loss: 0.8834 - val_acc: 0.7948\n",
      "Epoch 93/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0490 - acc: 0.9924 - val_loss: 0.8707 - val_acc: 0.7873\n",
      "Epoch 94/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0389 - acc: 0.9949 - val_loss: 0.8416 - val_acc: 0.7959\n",
      "Epoch 95/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0300 - acc: 0.9979 - val_loss: 0.8336 - val_acc: 0.8034\n",
      "Epoch 96/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0278 - acc: 0.9979 - val_loss: 0.8754 - val_acc: 0.7755\n",
      "Epoch 97/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0405 - acc: 0.9933 - val_loss: 0.8682 - val_acc: 0.7991\n",
      "Epoch 98/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0274 - acc: 0.9979 - val_loss: 0.9025 - val_acc: 0.7852\n",
      "Epoch 99/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0655 - acc: 0.9867 - val_loss: 0.8999 - val_acc: 0.7830\n",
      "Epoch 100/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.0822 - acc: 0.9834 - val_loss: 1.0730 - val_acc: 0.7508\n",
      "[3.8166569770342513, 2.163506687702367, 1.6736083667068726, 1.3836429419739149, 1.201780983395583, 1.0565181138654816, 0.93935571378461236, 0.84856165682890972, 0.81070141157575104, 0.74555983246607105, 0.69508815511162803, 0.65048271351786624, 0.60885509795106829, 0.5465333787351504, 0.55239360742836685, 0.54636913989196989, 0.48161712021417352, 0.45754080433424199, 0.43649897080821703, 0.43534384106251156, 0.40356407428029162, 0.4103384514558277, 0.35746272332757179, 0.34259220441392746, 0.32406453790337714, 0.3219029784504302, 0.30085345078902587, 0.28488513632573348, 0.28445119234860572, 0.27433159907622323, 0.26025146573740821, 0.24098748418520083, 0.2614096373772215, 0.23553397419043418, 0.22116397688438746, 0.23040689551989385, 0.21808025616684434, 0.20550366295300154, 0.19956364160685622, 0.19979467300901313, 0.17836072977874007, 0.16097926145811209, 0.16891111597398212, 0.17000145837150299, 0.15386495017821591, 0.14747491547494126, 0.13527099093532868, 0.13312448397299359, 0.13758230636910365, 0.12984704580440865, 0.14602750417717689, 0.13658778897447627, 0.13020695353700881, 0.11502738276413384, 0.14961183336518408, 0.11532461552701126, 0.1140150232964872, 0.10187236897157076, 0.10616896536717259, 0.10206495409208237, 0.10976017444118946, 0.09190399194924008, 0.090298566866102062, 0.1010724197279403, 0.083575431252343277, 0.097445884266044488, 0.093440548223226894, 0.074318660184007276, 0.084964775329038289, 0.080376200189362243, 0.066247286003304137, 0.060733377954074443, 0.084635999802889161, 0.096202274795589057, 0.064609981445853629, 0.067291791038651974, 0.11004398487854597, 0.08337790155899015, 0.091539765760007677, 0.13510240567098056, 0.089819081425941111, 0.080495771870970234, 0.053785556084718564, 0.044220951303078508, 0.039507878873132091, 0.040472429343428526, 0.053850500075102843, 0.05420376162751446, 0.041980486526036122, 0.042931988430747338, 0.046258400431356282, 0.047417151442100194, 0.048986168685414194, 0.038883794718831299, 0.029953677572388832, 0.027761497344089663, 0.040541218969160055, 0.027381209371156314, 0.065490494842662061, 0.082222267738477017]\n",
      "[0.22825586761519814, 0.5073630926771423, 0.60906580763920848, 0.66751035451340446, 0.70593649346434573, 0.73815002314681233, 0.77105384272361643, 0.79360331317218724, 0.78946157372387338, 0.80832949847161251, 0.82328578041162681, 0.83617119201031687, 0.84330418789600703, 0.86585365834457773, 0.85296824647159109, 0.8538886332533846, 0.87873907076615754, 0.88357109966993441, 0.88909341904407158, 0.8902439025761727, 0.8985273815825191, 0.89530602831254613, 0.91095260042119452, 0.91601472593813082, 0.92521859161655196, 0.91946617587612234, 0.92959042773288458, 0.93465255382584367, 0.92843994458479862, 0.93143120113189348, 0.93971467991880264, 0.94569719315014067, 0.93511274740874795, 0.94592728976330009, 0.95098941582882945, 0.94707777244508184, 0.94914864218295358, 0.95006902901960633, 0.95766221815904495, 0.95421076855020925, 0.96088357137415847, 0.96732627731065179, 0.96525540705161661, 0.96226415069652926, 0.97008743647655693, 0.9719282101224328, 0.9769903359410953, 0.97722043285598092, 0.97238840312931429, 0.97583985249128313, 0.96870685693474889, 0.97123792020066568, 0.97330878996596704, 0.98320294551129606, 0.96640589050142878, 0.97929130210011883, 0.97514956309067946, 0.98297284859641054, 0.97906120598069324, 0.97883110906580761, 0.97906120570639665, 0.98435342871424136, 0.98297284859641054, 0.98067188219052004, 0.98619420131779023, 0.9811320757459947, 0.98228255867464331, 0.98941555453290386, 0.98665439484583528, 0.98527381502973066, 0.99033594109526002, 0.9914864245725018, 0.98297284834954357, 0.98021168918363855, 0.9907961343764381, 0.99056603801014564, 0.97929130210011883, 0.98182236566776182, 0.98228255867464331, 0.96847676029415986, 0.98067188246481662, 0.98343304187758862, 0.99194661785367988, 0.9949378741813375, 0.99677864703175334, 0.99608835738428281, 0.99010584420780401, 0.98872526461113663, 0.99562816410310462, 0.9930971007823286, 0.99240681113485796, 0.99171652121309084, 0.99240681086056148, 0.99493787390704092, 0.99792913023469854, 0.99792913023469854, 0.99332719742291764, 0.99792913023469854, 0.98665439512013187, 0.98343304215188521]\n",
      "[2.8008812288208498, 1.9999221464745085, 1.7224564411458345, 1.4322786277398132, 1.3280504808774185, 1.2632088589489012, 1.2774087664392146, 1.1595635926300933, 1.1197834067390762, 1.0471981336171594, 1.0937263892621409, 1.0861570211537543, 0.97959856303242687, 1.0251483300062563, 1.0922064553270023, 1.0140613472423288, 0.95706733110511855, 0.9351937379027796, 1.0482664428387241, 0.97006926808014082, 0.94657115867135344, 0.90386696148635748, 0.98575421742030556, 0.87300903379725081, 1.0001763407577122, 0.92508210108948574, 0.89765932250099711, 0.87233005891425008, 0.92674126481651364, 0.83816924156615091, 0.87263227213338157, 0.86796070835388051, 0.90739067591862832, 0.88109368502005347, 0.89574487419312798, 0.83800833335368385, 0.90549761332195378, 0.8365676808946243, 0.80224049885214077, 0.84989299144191721, 0.87129436169223806, 0.84240450830900071, 0.9216618537902832, 0.86765491527082073, 0.88458905286614797, 0.80377609120275739, 0.85065589145738252, 0.87313411622503267, 0.83072775903244922, 0.87737389076921279, 0.9164183616126006, 0.85967389034021302, 0.82233365368254074, 0.82109061834251329, 0.83803937514537641, 0.88247247784529537, 0.85300377180957898, 0.79153083500109223, 0.89043600180223348, 0.88474716715090518, 0.84644419422211115, 0.79841187010518211, 0.86025664668078072, 0.9025761722622061, 0.89588038583319118, 0.88284139564034758, 0.87396674458373635, 0.87072884460268929, 0.8871119059502246, 0.83181964838927336, 0.80914924262545673, 0.81923587967578626, 0.94944739418562452, 0.90661618033490043, 0.85301368846032855, 0.91419829909962824, 0.89627125747221759, 0.90449259667852389, 0.96485905718982667, 1.0006925031526772, 0.90004440306592837, 0.9801593735958144, 0.85237674093400118, 0.85666885731939857, 0.89365788727646611, 0.91150299780606969, 0.88759614855339142, 0.86980464673580082, 0.88663549261881891, 0.8649120163072449, 0.9180811008500489, 0.883354504945839, 0.87066617649684785, 0.84159847181670522, 0.83360884320210948, 0.87542664287681871, 0.86820509968971715, 0.90252246956564042, 0.89994997829679002, 1.0729980626244551]\n",
      "[0.38453276076070986, 0.55316863677171324, 0.5778732556533609, 0.66165413629867853, 0.66702470538695569, 0.69602577956483391, 0.67239527479534378, 0.69495166558072086, 0.72610096759878096, 0.72932330903894282, 0.72180451198243945, 0.7089151458376588, 0.74113856126363242, 0.7336197645912621, 0.70247046282929071, 0.73469387831928645, 0.74973147243229321, 0.75725026916868576, 0.72287862603057473, 0.75080558603227332, 0.74758324452808922, 0.75510204152057048, 0.74221267524774548, 0.76369495236912044, 0.74543501687997393, 0.7583243830887767, 0.75617615537663918, 0.7593984969448454, 0.74865735844818015, 0.78088077406622025, 0.76476906628921126, 0.78302900197042435, 0.75939849707288976, 0.7733619770737391, 0.75402792766450166, 0.77443609092980781, 0.76476906616116702, 0.78302900165031342, 0.78517722955451741, 0.78302900165031342, 0.77980666027417378, 0.78517722942647306, 0.75725026929673012, 0.76799140772937324, 0.78088077387415378, 0.79377014033904525, 0.77336197713776123, 0.78195488773022248, 0.77658431857792309, 0.77336197700971687, 0.7679914078574176, 0.77658431870596745, 0.78410311557040435, 0.79162191275495208, 0.78732545733067705, 0.77765843249801403, 0.77551020484989874, 0.79914070961938888, 0.77658431857792309, 0.78732545726665493, 0.78732545720263281, 0.79162191262690773, 0.78839957105870151, 0.77013963544151065, 0.78732545733067705, 0.78195488792228907, 0.78625134328254187, 0.77765843262605838, 0.78625134328254187, 0.80773362053196107, 0.78947368485074809, 0.79484425413109183, 0.76476906628921126, 0.78302900190640212, 0.78195488785826683, 0.77765843256203626, 0.78839957112272363, 0.77765843249801403, 0.78302900171433565, 0.77980666027417378, 0.79806659569929805, 0.77228786315364817, 0.80343716497964168, 0.79484425419511395, 0.78732545720263281, 0.77980666027417378, 0.79269602648297643, 0.79162191262690773, 0.78302900171433565, 0.78517722949049529, 0.7905477987068168, 0.7948442540670696, 0.78732545720263281, 0.79591836805118266, 0.80343716497964168, 0.77551020484989874, 0.79914070949134453, 0.78517722949049529, 0.78302900184238, 0.75080558616031767]\n",
      "('Test loss:', 1.1036875795397123)\n",
      "('Test accuracy:', 0.75107296137339052)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_classification_layer = Dense(num_classes, activation='softmax')\n",
    "inp = vgg.input\n",
    "out = new_classification_layer(vgg.layers[-2].output)\n",
    "model_new = Model(inp, out)\n",
    "\n",
    "\n",
    "for l, layer in enumerate(model_new.layers[:-1]):\n",
    "    layer.trainable = False\n",
    "for l, layer in enumerate(model_new.layers[-1:]):\n",
    "    layer.trainable = True\n",
    "    \n",
    "model_new.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_new.summary()\n",
    "\n",
    "history = model_new.fit(x_train, y_train, \n",
    "                        batch_size=64, \n",
    "                        epochs=100, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "# with adam - 16\n",
    "print(history.history['loss'])\n",
    "print(history.history['acc'])\n",
    "\n",
    "print(history.history['val_loss'])\n",
    "print(history.history['val_acc'])\n",
    "\n",
    "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 97)                397409    \n",
      "=================================================================\n",
      "Total params: 134,657,953.0\n",
      "Trainable params: 397,409.0\n",
      "Non-trainable params: 134,260,544.0\n",
      "_________________________________________________________________\n",
      "Train on 4346 samples, validate on 931 samples\n",
      "Epoch 1/100\n",
      "4346/4346 [==============================] - 37s - loss: 4.2234 - acc: 0.0983 - val_loss: 3.8817 - val_acc: 0.1740\n",
      "Epoch 2/100\n",
      "4346/4346 [==============================] - 36s - loss: 3.3363 - acc: 0.2704 - val_loss: 3.3327 - val_acc: 0.2009\n",
      "Epoch 3/100\n",
      "4346/4346 [==============================] - 37s - loss: 2.7783 - acc: 0.3969 - val_loss: 2.8750 - val_acc: 0.2911\n",
      "Epoch 4/100\n",
      "4346/4346 [==============================] - 36s - loss: 2.4159 - acc: 0.4728 - val_loss: 2.4369 - val_acc: 0.4447\n",
      "Epoch 5/100\n",
      "4346/4346 [==============================] - 36s - loss: 2.1506 - acc: 0.5361 - val_loss: 2.1749 - val_acc: 0.5113\n",
      "Epoch 6/100\n",
      "4346/4346 [==============================] - 36s - loss: 1.9480 - acc: 0.5826 - val_loss: 2.0404 - val_acc: 0.5575\n",
      "Epoch 7/100\n",
      "4346/4346 [==============================] - 36s - loss: 1.7824 - acc: 0.6199 - val_loss: 1.9775 - val_acc: 0.5564\n",
      "Epoch 8/100\n",
      "4346/4346 [==============================] - 37s - loss: 1.6586 - acc: 0.6417 - val_loss: 1.8643 - val_acc: 0.4995\n",
      "Epoch 9/100\n",
      "4346/4346 [==============================] - 36s - loss: 1.5454 - acc: 0.6650 - val_loss: 1.7360 - val_acc: 0.5940\n",
      "Epoch 10/100\n",
      "4346/4346 [==============================] - 36s - loss: 1.4497 - acc: 0.6841 - val_loss: 1.6782 - val_acc: 0.5875\n",
      "Epoch 11/100\n",
      "4346/4346 [==============================] - 36s - loss: 1.3698 - acc: 0.7034 - val_loss: 1.5513 - val_acc: 0.6316\n",
      "Epoch 12/100\n",
      "4346/4346 [==============================] - 36s - loss: 1.2968 - acc: 0.7209 - val_loss: 1.5631 - val_acc: 0.6155\n",
      "Epoch 13/100\n",
      "4346/4346 [==============================] - 37s - loss: 1.2370 - acc: 0.7405 - val_loss: 1.4919 - val_acc: 0.6316\n",
      "Epoch 14/100\n",
      "4346/4346 [==============================] - 37s - loss: 1.1804 - acc: 0.7441 - val_loss: 1.4275 - val_acc: 0.6617\n",
      "Epoch 15/100\n",
      "4346/4346 [==============================] - 37s - loss: 1.1360 - acc: 0.7513 - val_loss: 1.4193 - val_acc: 0.6466\n",
      "Epoch 16/100\n",
      "4346/4346 [==============================] - 36s - loss: 1.0861 - acc: 0.7653 - val_loss: 1.3917 - val_acc: 0.6423\n",
      "Epoch 17/100\n",
      "4346/4346 [==============================] - 37s - loss: 1.0469 - acc: 0.7642 - val_loss: 1.3137 - val_acc: 0.6692\n",
      "Epoch 18/100\n",
      "4346/4346 [==============================] - 36s - loss: 1.0103 - acc: 0.7796 - val_loss: 1.3119 - val_acc: 0.6681\n",
      "Epoch 19/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.9736 - acc: 0.7927 - val_loss: 1.2898 - val_acc: 0.6788\n",
      "Epoch 20/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.9447 - acc: 0.7899 - val_loss: 1.2494 - val_acc: 0.6810\n",
      "Epoch 21/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.9111 - acc: 0.7973 - val_loss: 1.2256 - val_acc: 0.6885\n",
      "Epoch 22/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.8807 - acc: 0.8042 - val_loss: 1.2445 - val_acc: 0.6767\n",
      "Epoch 23/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.8634 - acc: 0.8122 - val_loss: 1.1735 - val_acc: 0.7025\n",
      "Epoch 24/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.8398 - acc: 0.8063 - val_loss: 1.1879 - val_acc: 0.6939\n",
      "Epoch 25/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.8174 - acc: 0.8185 - val_loss: 1.1240 - val_acc: 0.7143\n",
      "Epoch 26/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.7866 - acc: 0.8265 - val_loss: 1.1573 - val_acc: 0.7025\n",
      "Epoch 27/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.7825 - acc: 0.8254 - val_loss: 1.1020 - val_acc: 0.7068\n",
      "Epoch 28/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.7524 - acc: 0.8320 - val_loss: 1.1029 - val_acc: 0.7046\n",
      "Epoch 29/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.7375 - acc: 0.8366 - val_loss: 1.0939 - val_acc: 0.7068\n",
      "Epoch 30/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.7150 - acc: 0.8396 - val_loss: 1.0941 - val_acc: 0.6982\n",
      "Epoch 31/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.7018 - acc: 0.8387 - val_loss: 1.0750 - val_acc: 0.7240\n",
      "Epoch 32/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.6877 - acc: 0.8447 - val_loss: 1.0619 - val_acc: 0.7175\n",
      "Epoch 33/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.6751 - acc: 0.8468 - val_loss: 1.0743 - val_acc: 0.7175\n",
      "Epoch 34/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.6563 - acc: 0.8504 - val_loss: 1.1211 - val_acc: 0.7068\n",
      "Epoch 35/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.6465 - acc: 0.8566 - val_loss: 1.0255 - val_acc: 0.7411\n",
      "Epoch 36/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.6301 - acc: 0.8606 - val_loss: 1.0111 - val_acc: 0.7186\n",
      "Epoch 37/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.6164 - acc: 0.8638 - val_loss: 1.0069 - val_acc: 0.7401\n",
      "Epoch 38/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.6090 - acc: 0.8629 - val_loss: 0.9767 - val_acc: 0.7454\n",
      "Epoch 39/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.5976 - acc: 0.8636 - val_loss: 1.0306 - val_acc: 0.7325\n",
      "Epoch 40/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.5854 - acc: 0.8677 - val_loss: 0.9965 - val_acc: 0.7519\n",
      "Epoch 41/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.5751 - acc: 0.8728 - val_loss: 0.9862 - val_acc: 0.7358\n",
      "Epoch 42/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.5683 - acc: 0.8732 - val_loss: 0.9275 - val_acc: 0.7691\n",
      "Epoch 43/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.5583 - acc: 0.8737 - val_loss: 0.9625 - val_acc: 0.7519\n",
      "Epoch 44/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.5409 - acc: 0.8764 - val_loss: 0.9975 - val_acc: 0.7368\n",
      "Epoch 45/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.5341 - acc: 0.8829 - val_loss: 0.9179 - val_acc: 0.7626\n",
      "Epoch 46/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.5270 - acc: 0.8843 - val_loss: 0.9542 - val_acc: 0.7454\n",
      "Epoch 47/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.5145 - acc: 0.8900 - val_loss: 0.9669 - val_acc: 0.7293\n",
      "Epoch 48/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.5073 - acc: 0.8896 - val_loss: 0.9285 - val_acc: 0.7433\n",
      "Epoch 49/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.5029 - acc: 0.8875 - val_loss: 0.9204 - val_acc: 0.7583\n",
      "Epoch 50/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.4967 - acc: 0.8898 - val_loss: 0.9277 - val_acc: 0.7540\n",
      "Epoch 51/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4865 - acc: 0.8925 - val_loss: 0.9226 - val_acc: 0.7562\n",
      "Epoch 52/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4756 - acc: 0.8960 - val_loss: 0.9208 - val_acc: 0.7562\n",
      "Epoch 53/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4703 - acc: 0.8932 - val_loss: 0.9374 - val_acc: 0.7401\n",
      "Epoch 54/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4664 - acc: 0.8967 - val_loss: 0.9462 - val_acc: 0.7519\n",
      "Epoch 55/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4583 - acc: 0.8990 - val_loss: 0.9325 - val_acc: 0.7422\n",
      "Epoch 56/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4537 - acc: 0.9004 - val_loss: 0.9301 - val_acc: 0.7444\n",
      "Epoch 57/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4454 - acc: 0.9052 - val_loss: 0.9088 - val_acc: 0.7487\n",
      "Epoch 58/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4372 - acc: 0.9034 - val_loss: 0.8513 - val_acc: 0.7680\n",
      "Epoch 59/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4298 - acc: 0.9068 - val_loss: 0.8807 - val_acc: 0.7777\n",
      "Epoch 60/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.4235 - acc: 0.9075 - val_loss: 0.8626 - val_acc: 0.7626\n",
      "Epoch 61/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.4168 - acc: 0.9128 - val_loss: 0.8679 - val_acc: 0.7712\n",
      "Epoch 62/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4129 - acc: 0.9105 - val_loss: 0.9195 - val_acc: 0.7530\n",
      "Epoch 63/100\n",
      "4346/4346 [==============================] - 36s - loss: 0.4073 - acc: 0.9100 - val_loss: 0.8666 - val_acc: 0.7723\n",
      "Epoch 64/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3997 - acc: 0.9172 - val_loss: 0.8681 - val_acc: 0.7701\n",
      "Epoch 65/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3965 - acc: 0.9146 - val_loss: 0.8816 - val_acc: 0.7605\n",
      "Epoch 66/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3872 - acc: 0.9215 - val_loss: 0.8660 - val_acc: 0.7615\n",
      "Epoch 67/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3841 - acc: 0.9162 - val_loss: 0.8497 - val_acc: 0.7723\n",
      "Epoch 68/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3805 - acc: 0.9236 - val_loss: 0.8820 - val_acc: 0.7680\n",
      "Epoch 69/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3768 - acc: 0.9227 - val_loss: 0.9278 - val_acc: 0.7530\n",
      "Epoch 70/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3689 - acc: 0.9243 - val_loss: 0.8473 - val_acc: 0.7691\n",
      "Epoch 71/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3639 - acc: 0.9294 - val_loss: 0.8457 - val_acc: 0.7744\n",
      "Epoch 72/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3607 - acc: 0.9259 - val_loss: 0.8400 - val_acc: 0.7658\n",
      "Epoch 73/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3554 - acc: 0.9266 - val_loss: 0.8385 - val_acc: 0.7755\n",
      "Epoch 74/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3450 - acc: 0.9349 - val_loss: 0.8833 - val_acc: 0.7594\n",
      "Epoch 75/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3470 - acc: 0.9317 - val_loss: 0.8371 - val_acc: 0.7841\n",
      "Epoch 76/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3435 - acc: 0.9310 - val_loss: 0.8279 - val_acc: 0.7777\n",
      "Epoch 77/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3365 - acc: 0.9307 - val_loss: 0.8126 - val_acc: 0.7798\n",
      "Epoch 78/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3324 - acc: 0.9356 - val_loss: 0.8103 - val_acc: 0.7873\n",
      "Epoch 79/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3293 - acc: 0.9347 - val_loss: 0.8180 - val_acc: 0.7820\n",
      "Epoch 80/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3266 - acc: 0.9374 - val_loss: 0.8570 - val_acc: 0.7723\n",
      "Epoch 81/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3232 - acc: 0.9351 - val_loss: 0.8393 - val_acc: 0.7573\n",
      "Epoch 82/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3177 - acc: 0.9358 - val_loss: 0.8282 - val_acc: 0.7744\n",
      "Epoch 83/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3133 - acc: 0.9404 - val_loss: 0.8291 - val_acc: 0.7691\n",
      "Epoch 84/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3125 - acc: 0.9370 - val_loss: 0.8008 - val_acc: 0.7787\n",
      "Epoch 85/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3061 - acc: 0.9413 - val_loss: 0.8052 - val_acc: 0.7809\n",
      "Epoch 86/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.3005 - acc: 0.9418 - val_loss: 0.8281 - val_acc: 0.7755\n",
      "Epoch 87/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2972 - acc: 0.9411 - val_loss: 0.8233 - val_acc: 0.7820\n",
      "Epoch 88/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2935 - acc: 0.9450 - val_loss: 0.8028 - val_acc: 0.7777\n",
      "Epoch 89/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2925 - acc: 0.9420 - val_loss: 0.8149 - val_acc: 0.7820\n",
      "Epoch 90/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2912 - acc: 0.9434 - val_loss: 0.8190 - val_acc: 0.7830\n",
      "Epoch 91/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2867 - acc: 0.9462 - val_loss: 0.8377 - val_acc: 0.7766\n",
      "Epoch 92/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2831 - acc: 0.9452 - val_loss: 0.8030 - val_acc: 0.7841\n",
      "Epoch 93/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2779 - acc: 0.9468 - val_loss: 0.8106 - val_acc: 0.7809\n",
      "Epoch 94/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2766 - acc: 0.9517 - val_loss: 0.7985 - val_acc: 0.7852\n",
      "Epoch 95/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2744 - acc: 0.9494 - val_loss: 0.8086 - val_acc: 0.7820\n",
      "Epoch 96/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2754 - acc: 0.9432 - val_loss: 0.7964 - val_acc: 0.7798\n",
      "Epoch 97/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2659 - acc: 0.9498 - val_loss: 0.7974 - val_acc: 0.7744\n",
      "Epoch 98/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2635 - acc: 0.9524 - val_loss: 0.8172 - val_acc: 0.7841\n",
      "Epoch 99/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2647 - acc: 0.9505 - val_loss: 0.7996 - val_acc: 0.7884\n",
      "Epoch 100/100\n",
      "4346/4346 [==============================] - 37s - loss: 0.2552 - acc: 0.9567 - val_loss: 0.7793 - val_acc: 0.7948\n",
      "[4.2234452306209374, 3.3362512772828272, 2.7782996781564671, 2.4159013098195792, 2.1505896194816128, 1.9479523721072622, 1.7823660813110851, 1.6586137199752955, 1.5453591996275002, 1.4497494311909822, 1.3698088159442661, 1.2968014175860059, 1.2370157072031711, 1.1803647629741805, 1.1360248237011676, 1.0861407383981521, 1.0469416404615279, 1.0102747654070023, 0.97355958151477195, 0.94467005924974456, 0.91110028334054538, 0.88073388832371413, 0.86337233072483643, 0.8397536134588186, 0.81741345274355137, 0.78661339084398785, 0.78249219258809166, 0.75237180909087897, 0.73754657568385185, 0.71496357733458349, 0.70177691925301622, 0.6876572819552661, 0.67510289381683775, 0.65630706641403036, 0.64647050966494546, 0.63010392178602614, 0.61644920828810934, 0.60896711965557404, 0.59756386225200497, 0.58543438481889554, 0.57506053085926212, 0.56832936692775471, 0.55832210406695326, 0.54089002394204422, 0.53408592281065059, 0.52700123965767631, 0.51452523308089948, 0.50729397678506905, 0.50291685339661718, 0.49669366452313218, 0.48645038356223635, 0.47564886423282998, 0.47026053217770697, 0.46637760835806458, 0.45829650536145256, 0.45366838974882479, 0.44535389849791812, 0.43716449447891648, 0.42978677138501031, 0.42349393141198366, 0.41676853689739246, 0.4128853819417449, 0.40727369456427248, 0.39967647135888601, 0.39650767418957505, 0.38718837344640966, 0.38412830897371614, 0.38053977339979805, 0.37680588106488061, 0.36892052553679833, 0.36393061221276785, 0.36069678010833028, 0.35543827875056577, 0.34497818301418548, 0.34699592038173033, 0.34350047992551813, 0.33648708144531125, 0.33240677622732784, 0.32926852556757263, 0.32661692314689961, 0.3232092909751485, 0.31766194585167312, 0.3133382496886466, 0.31245933894841177, 0.30605230997824417, 0.30049131827808667, 0.29717858791954771, 0.29349710560865355, 0.2925498773302288, 0.29124448591396657, 0.28668296150606015, 0.28308435913828511, 0.27786922181908896, 0.27655073587542245, 0.27437892089886323, 0.27536909267806919, 0.26592540624390754, 0.26352589532520382, 0.26471082377258226, 0.25518894382764051]\n",
      "[0.09825126476863591, 0.27036355425562114, 0.39691670319203459, 0.47284859811113117, 0.53612516672996346, 0.58260468821124001, 0.61988034982918272, 0.64173953106915738, 0.66497928814793639, 0.68407731304726072, 0.70340543146019319, 0.72089277214023084, 0.74045099256996505, 0.74413253618614272, 0.75126553330616752, 0.76530142629744546, 0.76415094183273591, 0.77956741834312226, 0.79268292405887297, 0.78992176859597152, 0.79728486038165003, 0.80418775414081967, 0.8122411373568964, 0.80625862322037967, 0.8184537463785041, 0.82650713658914332, 0.82535664877801573, 0.83202945393348582, 0.83663138381029345, 0.83962264011052146, 0.83870225434362533, 0.84468475775514618, 0.84675563190919267, 0.85043717996897483, 0.85664979850867351, 0.86056143621448211, 0.86378278934730679, 0.86286240366269962, 0.86355269034776716, 0.867694431085275, 0.87275655808341268, 0.87321674801817262, 0.87367694747102354, 0.87643810090413032, 0.88288081441120903, 0.88426138429777779, 0.89001380895284588, 0.88955361739973615, 0.88748274129818405, 0.88978370978872845, 0.89254486626652707, 0.89599631806973534, 0.89323516159193672, 0.89668660500166986, 0.89898757993818368, 0.90036815725818953, 0.90520018742373065, 0.90335941081545179, 0.90681085971111641, 0.9075011519644044, 0.91279337787979276, 0.91049240656399366, 0.91003221775384968, 0.91716520971709881, 0.91463414658833031, 0.9215370432550436, 0.91624482556855247, 0.9236079163667632, 0.92268752648541852, 0.92429820941551133, 0.92936033666051587, 0.92590888727111742, 0.92659917499851208, 0.93488265488260747, 0.93166129555068034, 0.93097100562891322, 0.93074091365136591, 0.93557294535296776, 0.93465255725455076, 0.93741371535069917, 0.93511274581782788, 0.93580303927802078, 0.94040497294012104, 0.93695352387987851, 0.94132535483943558, 0.94178555525232444, 0.94109525929603277, 0.9450069071782442, 0.94201564942424443, 0.94339623085869884, 0.94615738783023129, 0.94523700069185235, 0.94684767821830262, 0.95167971162054321, 0.94937873918012816, 0.94316613153000339, 0.94983893443624157, 0.95236999781187703, 0.9505292197498264, 0.95674183376363164]\n",
      "[3.8816894721779991, 3.3327327921874543, 2.8750430236697579, 2.4368745689105271, 2.1749336834752855, 2.0403996563879683, 1.9774678442327087, 1.8643036101225496, 1.7359550529340668, 1.6781687260186249, 1.5513280973270551, 1.5631487510386137, 1.4918832545889948, 1.4274721451655878, 1.4192860499104418, 1.3916935183179877, 1.3136615965727449, 1.3118793608422694, 1.2898330504092699, 1.2494458473586627, 1.2256078654790155, 1.2445202068406709, 1.1735136531989634, 1.1878757798197959, 1.1239609361079019, 1.1573236265192739, 1.1019676309138953, 1.1028614433444277, 1.0938603145607559, 1.094123229432439, 1.0750279343345879, 1.0619080408559323, 1.0743315377373701, 1.1210611954923604, 1.0254597232369935, 1.0111204607274171, 1.0068938874276445, 0.97668266526866543, 1.0305585606397798, 0.99650824364478807, 0.98619604814961692, 0.92753314037200074, 0.962546251770249, 0.99754082567581681, 0.91792101327381892, 0.95424743958625835, 0.96694952716888849, 0.92848151621321728, 0.92042893420996907, 0.92772269863575285, 0.9226399281099199, 0.92084649579737543, 0.93737428554530811, 0.9461936642096711, 0.93250597291551007, 0.93013479041233738, 0.90883023615938507, 0.85127092419326245, 0.88073550681678614, 0.86259079133145411, 0.8678637923538749, 0.91952798855676043, 0.8666189869287575, 0.86805331425303167, 0.88161913191081376, 0.86595175089564669, 0.84968572067522974, 0.88202317880124459, 0.92782737553183681, 0.84730647291455952, 0.84566395328841326, 0.84004100782658697, 0.83850344897084794, 0.88327745912410516, 0.83708551196355441, 0.82794004838781632, 0.81258736069041082, 0.81026317609752163, 0.81795667411177297, 0.85699391838815364, 0.83928482732250409, 0.82824855306606926, 0.82910680284305494, 0.80080559110282956, 0.80518039827316068, 0.82806109101636582, 0.82331706617111555, 0.80276787345058553, 0.81493496843618685, 0.81899517268806732, 0.83774008645919162, 0.8029668817714769, 0.81061155224973225, 0.79848478560032832, 0.80862985594572234, 0.79638236537015528, 0.79736528079056457, 0.81720130566495575, 0.79964794725915922, 0.77929348348931005]\n",
      "[0.17400644356274836, 0.20085929076474413, 0.2910848584198158, 0.44468313497196077, 0.51127818473299647, 0.55746508771443593, 0.55639097488272204, 0.49946293839834688, 0.59398495357095493, 0.58754027459598379, 0.63157895191399549, 0.61546723824694638, 0.63157894749646537, 0.66165413681085583, 0.64661653552736553, 0.6423200910508825, 0.66917292861754096, 0.66809881578582697, 0.67883995492271398, 0.68098818257082927, 0.68850698519726183, 0.67669173124397353, 0.70247047761841308, 0.69387755377336169, 0.71428571876726654, 0.70247046481397812, 0.70676691614083398, 0.70461869687962353, 0.70676692452773882, 0.6981740198893398, 0.72395274065490955, 0.71750805726240841, 0.71750805924709582, 0.70676692696058152, 0.74113854794702005, 0.71858216369190486, 0.7400644415175236, 0.74543501848052829, 0.73254565611305611, 0.7518796934861246, 0.73576799460819786, 0.76906551556735747, 0.75187969547081202, 0.73684210942459927, 0.76262083857707386, 0.74543501009362345, 0.72932331563322683, 0.743286790832413, 0.75832437687862575, 0.75402792753645731, 0.75617614679766776, 0.7561761576174153, 0.74006443511530617, 0.75187969745549943, 0.742212673583169, 0.74436089284437945, 0.74865735697567015, 0.76799140516848619, 0.7776584270561292, 0.76262083659238644, 0.77121374564831546, 0.7529538083025259, 0.77228786289755946, 0.77013962641438405, 0.76047261092895857, 0.76154671735845503, 0.77228786488224688, 0.76799140075095618, 0.75295381272005601, 0.76906551998488759, 0.77443608856098733, 0.76584318148974573, 0.77551020337738874, 0.75939848971033963, 0.7841031188355353, 0.77765842904081661, 0.77980666110646202, 0.78732545733067705, 0.78195488676988989, 0.77228785649534204, 0.75725026603159917, 0.77443608414345733, 0.76906552638710501, 0.77873254385721802, 0.7808807739381759, 0.77551019697517121, 0.78195488478520248, 0.77765843544303415, 0.78195489317210742, 0.78302899761691647, 0.77658431422441521, 0.78410310404641292, 0.7808807527468361, 0.78517722526503175, 0.78195488478520248, 0.77980665470424448, 0.77443609054567475, 0.78410311243331776, 0.78839956574486103, 0.79484424715267477]\n",
      "('Test loss:', 0.80649440687613427)\n",
      "('Test accuracy:', 0.77467811158798283)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_classification_layer = Dense(num_classes, activation='softmax')\n",
    "inp = vgg.input\n",
    "out = new_classification_layer(vgg.layers[-2].output)\n",
    "model_new = Model(inp, out)\n",
    "\n",
    "\n",
    "for l, layer in enumerate(model_new.layers[:-1]):\n",
    "    layer.trainable = False\n",
    "for l, layer in enumerate(model_new.layers[-1:]):\n",
    "    layer.trainable = True\n",
    "    \n",
    "model_new.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adadelta',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model_new.summary()\n",
    "\n",
    "history = model_new.fit(x_train, y_train, \n",
    "                        batch_size=100, \n",
    "                        epochs=100, \n",
    "                        validation_data=(x_val, y_val))\n",
    "\n",
    "\n",
    "\n",
    "# with adam - 16\n",
    "print(history.history['loss'])\n",
    "print(history.history['acc'])\n",
    "\n",
    "print(history.history['val_loss'])\n",
    "print(history.history['val_acc'])\n",
    "\n",
    "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "matplotlib.pyplot.switch_backend('agg')\n",
    "\n",
    "matplotlib.pyplot.figure(figsize=(16,4))\n",
    "matplotlib.pyplot.plot(np.array([1,2,3,4,5]))\n",
    "matplotlib.pyplot.show()\n",
    "\n",
    "history.history['val_acc']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Improving results\n",
    "\n",
    "Can improve by doing augmentation on data\n",
    "running longer, trying different optimizers and hyperparams (keep val set)\n",
    "\n",
    "compare to imagenet results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4967 samples, validate on 621 samples\n",
      "Epoch 1/50\n",
      "4967/4967 [==============================] - 38s - loss: 4.0213 - acc: 0.1369 - val_loss: 3.4906 - val_acc: 0.1771\n",
      "Epoch 2/50\n",
      "4967/4967 [==============================] - 38s - loss: 2.9663 - acc: 0.3483 - val_loss: 2.8038 - val_acc: 0.3929\n",
      "Epoch 3/50\n",
      "4967/4967 [==============================] - 39s - loss: 2.4219 - acc: 0.4675 - val_loss: 2.3693 - val_acc: 0.4686\n",
      "Epoch 4/50\n",
      "4967/4967 [==============================] - 39s - loss: 2.0765 - acc: 0.5363 - val_loss: 2.0693 - val_acc: 0.5217\n",
      "Epoch 5/50\n",
      "4967/4967 [==============================] - 39s - loss: 1.8322 - acc: 0.5871 - val_loss: 1.9263 - val_acc: 0.5217\n",
      "Epoch 6/50\n",
      "4967/4967 [==============================] - 39s - loss: 1.6501 - acc: 0.6292 - val_loss: 1.6762 - val_acc: 0.5974\n",
      "Epoch 7/50\n",
      "4967/4967 [==============================] - 39s - loss: 1.5065 - acc: 0.6724 - val_loss: 1.6058 - val_acc: 0.6184\n",
      "Epoch 8/50\n",
      "4967/4967 [==============================] - 39s - loss: 1.3990 - acc: 0.6837 - val_loss: 1.5671 - val_acc: 0.6023\n",
      "Epoch 9/50\n",
      "4967/4967 [==============================] - 39s - loss: 1.3044 - acc: 0.7061 - val_loss: 1.4036 - val_acc: 0.6490\n",
      "Epoch 10/50\n",
      "4967/4967 [==============================] - 39s - loss: 1.2257 - acc: 0.7222 - val_loss: 1.3850 - val_acc: 0.6538\n",
      "Epoch 11/50\n",
      "4967/4967 [==============================] - 39s - loss: 1.1630 - acc: 0.7383 - val_loss: 1.3218 - val_acc: 0.6618\n",
      "Epoch 12/50\n",
      "4967/4967 [==============================] - 39s - loss: 1.0995 - acc: 0.7526 - val_loss: 1.2802 - val_acc: 0.6667\n",
      "Epoch 13/50\n",
      "4967/4967 [==============================] - 39s - loss: 1.0479 - acc: 0.7608 - val_loss: 1.2378 - val_acc: 0.6957\n",
      "Epoch 14/50\n",
      "4967/4967 [==============================] - 39s - loss: 1.0036 - acc: 0.7773 - val_loss: 1.2162 - val_acc: 0.6844\n",
      "Epoch 15/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.9637 - acc: 0.7812 - val_loss: 1.1700 - val_acc: 0.7037\n",
      "Epoch 16/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.9251 - acc: 0.7908 - val_loss: 1.1345 - val_acc: 0.7005\n",
      "Epoch 17/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.8921 - acc: 0.7969 - val_loss: 1.1129 - val_acc: 0.7198\n",
      "Epoch 18/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.8589 - acc: 0.8051 - val_loss: 1.0999 - val_acc: 0.7230\n",
      "Epoch 19/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.8360 - acc: 0.8049 - val_loss: 1.0576 - val_acc: 0.7182\n",
      "Epoch 20/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.8075 - acc: 0.8132 - val_loss: 1.0262 - val_acc: 0.7488\n",
      "Epoch 21/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.7856 - acc: 0.8172 - val_loss: 1.0175 - val_acc: 0.7343\n",
      "Epoch 22/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.7583 - acc: 0.8226 - val_loss: 1.0068 - val_acc: 0.7520\n",
      "Epoch 23/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.7344 - acc: 0.8359 - val_loss: 1.0053 - val_acc: 0.7375\n",
      "Epoch 24/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.7179 - acc: 0.8353 - val_loss: 0.9870 - val_acc: 0.7440\n",
      "Epoch 25/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.6990 - acc: 0.8395 - val_loss: 0.9542 - val_acc: 0.7520\n",
      "Epoch 26/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.6814 - acc: 0.8448 - val_loss: 0.9865 - val_acc: 0.7327\n",
      "Epoch 27/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.6604 - acc: 0.8490 - val_loss: 0.9525 - val_acc: 0.7472\n",
      "Epoch 28/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.6469 - acc: 0.8512 - val_loss: 0.9452 - val_acc: 0.7520\n",
      "Epoch 29/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.6296 - acc: 0.8573 - val_loss: 0.9191 - val_acc: 0.7681\n",
      "Epoch 30/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.6208 - acc: 0.8563 - val_loss: 0.8844 - val_acc: 0.7697\n",
      "Epoch 31/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.6059 - acc: 0.8546 - val_loss: 0.8938 - val_acc: 0.7746\n",
      "Epoch 32/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.5905 - acc: 0.8667 - val_loss: 0.9023 - val_acc: 0.7552\n",
      "Epoch 33/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.5799 - acc: 0.8651 - val_loss: 0.8761 - val_acc: 0.7633\n",
      "Epoch 34/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.5689 - acc: 0.8669 - val_loss: 0.8825 - val_acc: 0.7585\n",
      "Epoch 35/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.5563 - acc: 0.8760 - val_loss: 0.8861 - val_acc: 0.7649\n",
      "Epoch 36/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.5420 - acc: 0.8800 - val_loss: 0.8353 - val_acc: 0.7729\n",
      "Epoch 37/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.5322 - acc: 0.8834 - val_loss: 0.8381 - val_acc: 0.7874\n",
      "Epoch 38/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.5215 - acc: 0.8792 - val_loss: 0.8128 - val_acc: 0.7810\n",
      "Epoch 39/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.5101 - acc: 0.8848 - val_loss: 0.8104 - val_acc: 0.7955\n",
      "Epoch 40/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.5054 - acc: 0.8836 - val_loss: 0.8401 - val_acc: 0.7794\n",
      "Epoch 41/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.4912 - acc: 0.8885 - val_loss: 0.8094 - val_acc: 0.7939\n",
      "Epoch 42/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.4896 - acc: 0.8852 - val_loss: 0.8082 - val_acc: 0.7842\n",
      "Epoch 43/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.4763 - acc: 0.8953 - val_loss: 0.7841 - val_acc: 0.7955\n",
      "Epoch 44/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.4648 - acc: 0.8983 - val_loss: 0.7797 - val_acc: 0.7971\n",
      "Epoch 45/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.4588 - acc: 0.8985 - val_loss: 0.8594 - val_acc: 0.7601\n",
      "Epoch 46/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.4482 - acc: 0.8979 - val_loss: 0.7935 - val_acc: 0.7826\n",
      "Epoch 47/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.4443 - acc: 0.9020 - val_loss: 0.8242 - val_acc: 0.7697\n",
      "Epoch 48/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.4347 - acc: 0.9044 - val_loss: 0.8201 - val_acc: 0.7762\n",
      "Epoch 49/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.4321 - acc: 0.9007 - val_loss: 0.8121 - val_acc: 0.7826\n",
      "Epoch 50/50\n",
      "4967/4967 [==============================] - 39s - loss: 0.4244 - acc: 0.9072 - val_loss: 0.7476 - val_acc: 0.8068\n"
     ]
    }
   ],
   "source": [
    "history = model_new.fit(x_train, y_train, \n",
    "                        batch_size=64, \n",
    "                        epochs=50, \n",
    "                        validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Test loss:', 5.8760007768229867)\n",
      "('Test accuracy:', 0.53218884120171672)\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model_new.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
